{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic and Softmax Regression for Handwritten Digits Classification \n",
    "Programming Assignment 1\n",
    "CSE 253: Neural Networks for Pattern Recognition, by Prof. Gary Cottrell\n",
    "\n",
    "Shilin Zhu\n",
    "Ph.D. student, Computer Science, UCSD\n",
    "shz338@eng.ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "Run the code below to import the packages we need in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MNIST\n",
    "MNIST is a handwritten digit database by Yann LeCun. To reduce the computation, we only use 20000 images for training and 2000 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d7b92b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10898cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('./python-mnist/data')\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "plt.imshow(np.array(train_images[0]).reshape([28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We only use 20000 images for training and 2000 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "test_images = np.array(test_images)\n",
    "train_labels = np.array(train_labels).reshape([60000,1])\n",
    "test_labels = np.array(test_labels).reshape([10000,1])\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1)\n"
     ]
    }
   ],
   "source": [
    "print((train_images[1,:]).reshape(784,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to only retain the examples with two labels to perform binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "test_size = 1000\n",
    "train_images_sub = np.zeros([784,train_size])\n",
    "train_labels_sub = np.zeros([1,train_size])\n",
    "test_images_sub = np.zeros([784,test_size])\n",
    "test_labels_sub = np.zeros([1,test_size])\n",
    "counter = 0\n",
    "for i in range(0,train_size):\n",
    "    if train_labels[i][0] == 1 or train_labels[i][0] == 2:\n",
    "        train_images_sub[:,counter] = train_images[i,:]\n",
    "        train_labels_sub[:,counter] = train_labels[i,:]-1\n",
    "        counter = counter + 1        \n",
    "train_images_sub = train_images_sub[:,0:counter-1]\n",
    "train_labels_sub = train_labels_sub[:,0:counter-1]\n",
    "\n",
    "counter = 0\n",
    "for i in range(0,test_size):\n",
    "    if test_labels[i][0] == 1 or test_labels[i][0] == 2:\n",
    "        test_images_sub[:,counter] = test_images[i,:]\n",
    "        test_labels_sub[:,counter] = test_labels[i,:]-1\n",
    "        counter = counter + 1        \n",
    "test_images_sub = test_images_sub[:,0:counter-1]\n",
    "test_labels_sub = test_labels_sub[:,0:counter-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 214)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10b110a90>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d82e9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_images_sub.shape)\n",
    "plt.imshow(np.array(train_images_sub[:,5]).reshape([28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize the features (already flattened) by dividing over 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_images_sub/255.\n",
    "test_set_x = test_images_sub/255.\n",
    "train_set_y = train_labels_sub\n",
    "test_set_y = test_labels_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.48627451  0.99215686\n",
      "  1.          0.24705882  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.37647059  0.95686275\n",
      "  0.98431373  0.99215686  0.24313725  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.49803922\n",
      "  0.98431373  0.98431373  0.99215686  0.24313725  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.26666667\n",
      "  0.9254902   0.98431373  0.82745098  0.12156863  0.03137255  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.23529412\n",
      "  0.89411765  0.98431373  0.98431373  0.36862745  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.60784314  0.99215686  0.99215686  0.74117647  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.07843137  0.99215686  0.98431373  0.92156863  0.25882353  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.1254902\n",
      "  0.80392157  0.99215686  0.98431373  0.49411765  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.40784314  0.98431373  0.99215686  0.72156863  0.05882353  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.31372549  0.94117647  0.98431373  0.75686275  0.09019608  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.1254902\n",
      "  0.99215686  0.99215686  0.99215686  0.62352941  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.59215686  0.98431373  0.98431373  0.98431373  0.15294118  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.18823529  0.86666667  0.98431373  0.98431373  0.6745098   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.91764706  0.98431373  0.98431373  0.76862745  0.04705882  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.99215686  0.98431373  0.98431373  0.34901961  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.62352941  1.          0.99215686  0.99215686  0.12156863  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.18823529  0.89411765  0.99215686  0.96862745  0.54901961  0.03137255\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.25098039  0.98431373  0.99215686  0.8627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.25098039  0.98431373  0.99215686  0.8627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.09411765  0.75686275  0.99215686  0.8627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_set_x[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we perform the following steps:\n",
    "-initialize the parameters\n",
    "-learn the parameters using logistic/softmax regression\n",
    "-use the learned parameters to make predictions\n",
    "-analyze and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build some helper functions so that we will build the entire model later based on these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    a = 1/(1+np.exp(-z))  \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic/softmax regression, zero initialization can work reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros([dim,1])\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a simple one-layer neural network, so we reuse the term forward propagation and backward propagation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    cost = -(1/m)*np.sum(np.multiply(Y,np.log(A))+np.multiply(1-Y,np.log(1-A)),keepdims=True)                                 # compute cost\n",
    "\n",
    "    dw = (1/m)*(np.dot(X,(A-Y).T))\n",
    "    \n",
    "    db = (1/m)*np.sum(A-Y,keepdims=True)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we compute the gradients, we can use gradient descent to optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the learned parameters to predict labels for the unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        Y_prediction[0,i] = np.where(A[0,i]>0.5,1,0)\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Entire Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our implemented helper functions, we can now build the entire model by combining them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = True):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    w, b = np.zeros((X_train.shape[0],1)), 0.\n",
    "    \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    print(b)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now use the following code to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 1: 0.686065\n",
      "Cost after iteration 2: 0.679138\n",
      "Cost after iteration 3: 0.672362\n",
      "Cost after iteration 4: 0.665732\n",
      "Cost after iteration 5: 0.659243\n",
      "Cost after iteration 6: 0.652890\n",
      "Cost after iteration 7: 0.646670\n",
      "Cost after iteration 8: 0.640578\n",
      "Cost after iteration 9: 0.634610\n",
      "Cost after iteration 10: 0.628763\n",
      "Cost after iteration 11: 0.623033\n",
      "Cost after iteration 12: 0.617416\n",
      "Cost after iteration 13: 0.611911\n",
      "Cost after iteration 14: 0.606512\n",
      "Cost after iteration 15: 0.601218\n",
      "Cost after iteration 16: 0.596025\n",
      "Cost after iteration 17: 0.590930\n",
      "Cost after iteration 18: 0.585932\n",
      "Cost after iteration 19: 0.581028\n",
      "Cost after iteration 20: 0.576214\n",
      "Cost after iteration 21: 0.571489\n",
      "Cost after iteration 22: 0.566850\n",
      "Cost after iteration 23: 0.562296\n",
      "Cost after iteration 24: 0.557823\n",
      "Cost after iteration 25: 0.553431\n",
      "Cost after iteration 26: 0.549116\n",
      "Cost after iteration 27: 0.544878\n",
      "Cost after iteration 28: 0.540714\n",
      "Cost after iteration 29: 0.536622\n",
      "Cost after iteration 30: 0.532601\n",
      "Cost after iteration 31: 0.528649\n",
      "Cost after iteration 32: 0.524765\n",
      "Cost after iteration 33: 0.520946\n",
      "Cost after iteration 34: 0.517192\n",
      "Cost after iteration 35: 0.513500\n",
      "Cost after iteration 36: 0.509870\n",
      "Cost after iteration 37: 0.506300\n",
      "Cost after iteration 38: 0.502788\n",
      "Cost after iteration 39: 0.499334\n",
      "Cost after iteration 40: 0.495936\n",
      "Cost after iteration 41: 0.492592\n",
      "Cost after iteration 42: 0.489302\n",
      "Cost after iteration 43: 0.486064\n",
      "Cost after iteration 44: 0.482878\n",
      "Cost after iteration 45: 0.479741\n",
      "Cost after iteration 46: 0.476654\n",
      "Cost after iteration 47: 0.473614\n",
      "Cost after iteration 48: 0.470621\n",
      "Cost after iteration 49: 0.467675\n",
      "Cost after iteration 50: 0.464773\n",
      "Cost after iteration 51: 0.461915\n",
      "Cost after iteration 52: 0.459100\n",
      "Cost after iteration 53: 0.456327\n",
      "Cost after iteration 54: 0.453595\n",
      "Cost after iteration 55: 0.450904\n",
      "Cost after iteration 56: 0.448253\n",
      "Cost after iteration 57: 0.445640\n",
      "Cost after iteration 58: 0.443064\n",
      "Cost after iteration 59: 0.440527\n",
      "Cost after iteration 60: 0.438025\n",
      "Cost after iteration 61: 0.435559\n",
      "Cost after iteration 62: 0.433128\n",
      "Cost after iteration 63: 0.430731\n",
      "Cost after iteration 64: 0.428368\n",
      "Cost after iteration 65: 0.426038\n",
      "Cost after iteration 66: 0.423739\n",
      "Cost after iteration 67: 0.421473\n",
      "Cost after iteration 68: 0.419237\n",
      "Cost after iteration 69: 0.417032\n",
      "Cost after iteration 70: 0.414856\n",
      "Cost after iteration 71: 0.412710\n",
      "Cost after iteration 72: 0.410592\n",
      "Cost after iteration 73: 0.408502\n",
      "Cost after iteration 74: 0.406440\n",
      "Cost after iteration 75: 0.404404\n",
      "Cost after iteration 76: 0.402396\n",
      "Cost after iteration 77: 0.400413\n",
      "Cost after iteration 78: 0.398455\n",
      "Cost after iteration 79: 0.396523\n",
      "Cost after iteration 80: 0.394615\n",
      "Cost after iteration 81: 0.392732\n",
      "Cost after iteration 82: 0.390871\n",
      "Cost after iteration 83: 0.389035\n",
      "Cost after iteration 84: 0.387220\n",
      "Cost after iteration 85: 0.385429\n",
      "Cost after iteration 86: 0.383659\n",
      "Cost after iteration 87: 0.381911\n",
      "Cost after iteration 88: 0.380184\n",
      "Cost after iteration 89: 0.378477\n",
      "Cost after iteration 90: 0.376792\n",
      "Cost after iteration 91: 0.375126\n",
      "Cost after iteration 92: 0.373480\n",
      "Cost after iteration 93: 0.371853\n",
      "Cost after iteration 94: 0.370245\n",
      "Cost after iteration 95: 0.368656\n",
      "Cost after iteration 96: 0.367085\n",
      "Cost after iteration 97: 0.365533\n",
      "Cost after iteration 98: 0.363998\n",
      "Cost after iteration 99: 0.362480\n",
      "Cost after iteration 100: 0.360980\n",
      "Cost after iteration 101: 0.359496\n",
      "Cost after iteration 102: 0.358029\n",
      "Cost after iteration 103: 0.356579\n",
      "Cost after iteration 104: 0.355144\n",
      "Cost after iteration 105: 0.353725\n",
      "Cost after iteration 106: 0.352322\n",
      "Cost after iteration 107: 0.350933\n",
      "Cost after iteration 108: 0.349560\n",
      "Cost after iteration 109: 0.348202\n",
      "Cost after iteration 110: 0.346858\n",
      "Cost after iteration 111: 0.345528\n",
      "Cost after iteration 112: 0.344212\n",
      "Cost after iteration 113: 0.342910\n",
      "Cost after iteration 114: 0.341622\n",
      "Cost after iteration 115: 0.340347\n",
      "Cost after iteration 116: 0.339085\n",
      "Cost after iteration 117: 0.337836\n",
      "Cost after iteration 118: 0.336600\n",
      "Cost after iteration 119: 0.335376\n",
      "Cost after iteration 120: 0.334165\n",
      "Cost after iteration 121: 0.332965\n",
      "Cost after iteration 122: 0.331778\n",
      "Cost after iteration 123: 0.330603\n",
      "Cost after iteration 124: 0.329439\n",
      "Cost after iteration 125: 0.328287\n",
      "Cost after iteration 126: 0.327145\n",
      "Cost after iteration 127: 0.326015\n",
      "Cost after iteration 128: 0.324896\n",
      "Cost after iteration 129: 0.323788\n",
      "Cost after iteration 130: 0.322690\n",
      "Cost after iteration 131: 0.321603\n",
      "Cost after iteration 132: 0.320526\n",
      "Cost after iteration 133: 0.319459\n",
      "Cost after iteration 134: 0.318402\n",
      "Cost after iteration 135: 0.317355\n",
      "Cost after iteration 136: 0.316317\n",
      "Cost after iteration 137: 0.315289\n",
      "Cost after iteration 138: 0.314271\n",
      "Cost after iteration 139: 0.313262\n",
      "Cost after iteration 140: 0.312262\n",
      "Cost after iteration 141: 0.311271\n",
      "Cost after iteration 142: 0.310289\n",
      "Cost after iteration 143: 0.309316\n",
      "Cost after iteration 144: 0.308351\n",
      "Cost after iteration 145: 0.307395\n",
      "Cost after iteration 146: 0.306447\n",
      "Cost after iteration 147: 0.305508\n",
      "Cost after iteration 148: 0.304576\n",
      "Cost after iteration 149: 0.303653\n",
      "Cost after iteration 150: 0.302738\n",
      "Cost after iteration 151: 0.301831\n",
      "Cost after iteration 152: 0.300931\n",
      "Cost after iteration 153: 0.300039\n",
      "Cost after iteration 154: 0.299155\n",
      "Cost after iteration 155: 0.298278\n",
      "Cost after iteration 156: 0.297408\n",
      "Cost after iteration 157: 0.296546\n",
      "Cost after iteration 158: 0.295691\n",
      "Cost after iteration 159: 0.294843\n",
      "Cost after iteration 160: 0.294001\n",
      "Cost after iteration 161: 0.293167\n",
      "Cost after iteration 162: 0.292339\n",
      "Cost after iteration 163: 0.291519\n",
      "Cost after iteration 164: 0.290704\n",
      "Cost after iteration 165: 0.289897\n",
      "Cost after iteration 166: 0.289095\n",
      "Cost after iteration 167: 0.288300\n",
      "Cost after iteration 168: 0.287512\n",
      "Cost after iteration 169: 0.286729\n",
      "Cost after iteration 170: 0.285953\n",
      "Cost after iteration 171: 0.285183\n",
      "Cost after iteration 172: 0.284419\n",
      "Cost after iteration 173: 0.283660\n",
      "Cost after iteration 174: 0.282908\n",
      "Cost after iteration 175: 0.282161\n",
      "Cost after iteration 176: 0.281420\n",
      "Cost after iteration 177: 0.280684\n",
      "Cost after iteration 178: 0.279954\n",
      "Cost after iteration 179: 0.279230\n",
      "Cost after iteration 180: 0.278511\n",
      "Cost after iteration 181: 0.277797\n",
      "Cost after iteration 182: 0.277089\n",
      "Cost after iteration 183: 0.276386\n",
      "Cost after iteration 184: 0.275688\n",
      "Cost after iteration 185: 0.274995\n",
      "Cost after iteration 186: 0.274307\n",
      "Cost after iteration 187: 0.273624\n",
      "Cost after iteration 188: 0.272946\n",
      "Cost after iteration 189: 0.272273\n",
      "Cost after iteration 190: 0.271605\n",
      "Cost after iteration 191: 0.270941\n",
      "Cost after iteration 192: 0.270283\n",
      "Cost after iteration 193: 0.269628\n",
      "Cost after iteration 194: 0.268979\n",
      "Cost after iteration 195: 0.268334\n",
      "Cost after iteration 196: 0.267693\n",
      "Cost after iteration 197: 0.267057\n",
      "Cost after iteration 198: 0.266426\n",
      "Cost after iteration 199: 0.265798\n",
      "Cost after iteration 200: 0.265175\n",
      "Cost after iteration 201: 0.264556\n",
      "Cost after iteration 202: 0.263942\n",
      "Cost after iteration 203: 0.263331\n",
      "Cost after iteration 204: 0.262725\n",
      "Cost after iteration 205: 0.262123\n",
      "Cost after iteration 206: 0.261525\n",
      "Cost after iteration 207: 0.260930\n",
      "Cost after iteration 208: 0.260340\n",
      "Cost after iteration 209: 0.259754\n",
      "Cost after iteration 210: 0.259171\n",
      "Cost after iteration 211: 0.258592\n",
      "Cost after iteration 212: 0.258017\n",
      "Cost after iteration 213: 0.257446\n",
      "Cost after iteration 214: 0.256879\n",
      "Cost after iteration 215: 0.256315\n",
      "Cost after iteration 216: 0.255754\n",
      "Cost after iteration 217: 0.255198\n",
      "Cost after iteration 218: 0.254644\n",
      "Cost after iteration 219: 0.254095\n",
      "Cost after iteration 220: 0.253549\n",
      "Cost after iteration 221: 0.253006\n",
      "Cost after iteration 222: 0.252466\n",
      "Cost after iteration 223: 0.251930\n",
      "Cost after iteration 224: 0.251398\n",
      "Cost after iteration 225: 0.250868\n",
      "Cost after iteration 226: 0.250342\n",
      "Cost after iteration 227: 0.249819\n",
      "Cost after iteration 228: 0.249300\n",
      "Cost after iteration 229: 0.248783\n",
      "Cost after iteration 230: 0.248270\n",
      "Cost after iteration 231: 0.247759\n",
      "Cost after iteration 232: 0.247252\n",
      "Cost after iteration 233: 0.246748\n",
      "Cost after iteration 234: 0.246247\n",
      "Cost after iteration 235: 0.245749\n",
      "Cost after iteration 236: 0.245253\n",
      "Cost after iteration 237: 0.244761\n",
      "Cost after iteration 238: 0.244272\n",
      "Cost after iteration 239: 0.243785\n",
      "Cost after iteration 240: 0.243301\n",
      "Cost after iteration 241: 0.242821\n",
      "Cost after iteration 242: 0.242342\n",
      "Cost after iteration 243: 0.241867\n",
      "Cost after iteration 244: 0.241394\n",
      "Cost after iteration 245: 0.240925\n",
      "Cost after iteration 246: 0.240457\n",
      "Cost after iteration 247: 0.239993\n",
      "Cost after iteration 248: 0.239531\n",
      "Cost after iteration 249: 0.239072\n",
      "Cost after iteration 250: 0.238615\n",
      "Cost after iteration 251: 0.238161\n",
      "Cost after iteration 252: 0.237709\n",
      "Cost after iteration 253: 0.237260\n",
      "Cost after iteration 254: 0.236813\n",
      "Cost after iteration 255: 0.236369\n",
      "Cost after iteration 256: 0.235927\n",
      "Cost after iteration 257: 0.235488\n",
      "Cost after iteration 258: 0.235051\n",
      "Cost after iteration 259: 0.234617\n",
      "Cost after iteration 260: 0.234185\n",
      "Cost after iteration 261: 0.233755\n",
      "Cost after iteration 262: 0.233327\n",
      "Cost after iteration 263: 0.232902\n",
      "Cost after iteration 264: 0.232479\n",
      "Cost after iteration 265: 0.232058\n",
      "Cost after iteration 266: 0.231640\n",
      "Cost after iteration 267: 0.231224\n",
      "Cost after iteration 268: 0.230810\n",
      "Cost after iteration 269: 0.230398\n",
      "Cost after iteration 270: 0.229988\n",
      "Cost after iteration 271: 0.229581\n",
      "Cost after iteration 272: 0.229175\n",
      "Cost after iteration 273: 0.228772\n",
      "Cost after iteration 274: 0.228371\n",
      "Cost after iteration 275: 0.227972\n",
      "Cost after iteration 276: 0.227574\n",
      "Cost after iteration 277: 0.227179\n",
      "Cost after iteration 278: 0.226786\n",
      "Cost after iteration 279: 0.226395\n",
      "Cost after iteration 280: 0.226006\n",
      "Cost after iteration 281: 0.225619\n",
      "Cost after iteration 282: 0.225234\n",
      "Cost after iteration 283: 0.224851\n",
      "Cost after iteration 284: 0.224469\n",
      "Cost after iteration 285: 0.224090\n",
      "Cost after iteration 286: 0.223712\n",
      "Cost after iteration 287: 0.223337\n",
      "Cost after iteration 288: 0.222963\n",
      "Cost after iteration 289: 0.222591\n",
      "Cost after iteration 290: 0.222221\n",
      "Cost after iteration 291: 0.221853\n",
      "Cost after iteration 292: 0.221486\n",
      "Cost after iteration 293: 0.221121\n",
      "Cost after iteration 294: 0.220758\n",
      "Cost after iteration 295: 0.220397\n",
      "Cost after iteration 296: 0.220038\n",
      "Cost after iteration 297: 0.219680\n",
      "Cost after iteration 298: 0.219324\n",
      "Cost after iteration 299: 0.218969\n",
      "Cost after iteration 300: 0.218617\n",
      "Cost after iteration 301: 0.218266\n",
      "Cost after iteration 302: 0.217916\n",
      "Cost after iteration 303: 0.217569\n",
      "Cost after iteration 304: 0.217223\n",
      "Cost after iteration 305: 0.216878\n",
      "Cost after iteration 306: 0.216535\n",
      "Cost after iteration 307: 0.216194\n",
      "Cost after iteration 308: 0.215854\n",
      "Cost after iteration 309: 0.215516\n",
      "Cost after iteration 310: 0.215180\n",
      "Cost after iteration 311: 0.214845\n",
      "Cost after iteration 312: 0.214511\n",
      "Cost after iteration 313: 0.214179\n",
      "Cost after iteration 314: 0.213849\n",
      "Cost after iteration 315: 0.213520\n",
      "Cost after iteration 316: 0.213192\n",
      "Cost after iteration 317: 0.212866\n",
      "Cost after iteration 318: 0.212542\n",
      "Cost after iteration 319: 0.212219\n",
      "Cost after iteration 320: 0.211897\n",
      "Cost after iteration 321: 0.211577\n",
      "Cost after iteration 322: 0.211258\n",
      "Cost after iteration 323: 0.210941\n",
      "Cost after iteration 324: 0.210625\n",
      "Cost after iteration 325: 0.210310\n",
      "Cost after iteration 326: 0.209997\n",
      "Cost after iteration 327: 0.209685\n",
      "Cost after iteration 328: 0.209375\n",
      "Cost after iteration 329: 0.209066\n",
      "Cost after iteration 330: 0.208758\n",
      "Cost after iteration 331: 0.208452\n",
      "Cost after iteration 332: 0.208147\n",
      "Cost after iteration 333: 0.207843\n",
      "Cost after iteration 334: 0.207540\n",
      "Cost after iteration 335: 0.207239\n",
      "Cost after iteration 336: 0.206939\n",
      "Cost after iteration 337: 0.206641\n",
      "Cost after iteration 338: 0.206343\n",
      "Cost after iteration 339: 0.206047\n",
      "Cost after iteration 340: 0.205752\n",
      "Cost after iteration 341: 0.205459\n",
      "Cost after iteration 342: 0.205166\n",
      "Cost after iteration 343: 0.204875\n",
      "Cost after iteration 344: 0.204585\n",
      "Cost after iteration 345: 0.204296\n",
      "Cost after iteration 346: 0.204009\n",
      "Cost after iteration 347: 0.203722\n",
      "Cost after iteration 348: 0.203437\n",
      "Cost after iteration 349: 0.203153\n",
      "Cost after iteration 350: 0.202870\n",
      "Cost after iteration 351: 0.202589\n",
      "Cost after iteration 352: 0.202308\n",
      "Cost after iteration 353: 0.202029\n",
      "Cost after iteration 354: 0.201750\n",
      "Cost after iteration 355: 0.201473\n",
      "Cost after iteration 356: 0.201197\n",
      "Cost after iteration 357: 0.200922\n",
      "Cost after iteration 358: 0.200648\n",
      "Cost after iteration 359: 0.200376\n",
      "Cost after iteration 360: 0.200104\n",
      "Cost after iteration 361: 0.199833\n",
      "Cost after iteration 362: 0.199564\n",
      "Cost after iteration 363: 0.199295\n",
      "Cost after iteration 364: 0.199028\n",
      "Cost after iteration 365: 0.198762\n",
      "Cost after iteration 366: 0.198496\n",
      "Cost after iteration 367: 0.198232\n",
      "Cost after iteration 368: 0.197969\n",
      "Cost after iteration 369: 0.197707\n",
      "Cost after iteration 370: 0.197446\n",
      "Cost after iteration 371: 0.197186\n",
      "Cost after iteration 372: 0.196926\n",
      "Cost after iteration 373: 0.196668\n",
      "Cost after iteration 374: 0.196411\n",
      "Cost after iteration 375: 0.196155\n",
      "Cost after iteration 376: 0.195900\n",
      "Cost after iteration 377: 0.195645\n",
      "Cost after iteration 378: 0.195392\n",
      "Cost after iteration 379: 0.195140\n",
      "Cost after iteration 380: 0.194889\n",
      "Cost after iteration 381: 0.194638\n",
      "Cost after iteration 382: 0.194389\n",
      "Cost after iteration 383: 0.194140\n",
      "Cost after iteration 384: 0.193893\n",
      "Cost after iteration 385: 0.193646\n",
      "Cost after iteration 386: 0.193400\n",
      "Cost after iteration 387: 0.193155\n",
      "Cost after iteration 388: 0.192912\n",
      "Cost after iteration 389: 0.192669\n",
      "Cost after iteration 390: 0.192426\n",
      "Cost after iteration 391: 0.192185\n",
      "Cost after iteration 392: 0.191945\n",
      "Cost after iteration 393: 0.191705\n",
      "Cost after iteration 394: 0.191467\n",
      "Cost after iteration 395: 0.191229\n",
      "Cost after iteration 396: 0.190992\n",
      "Cost after iteration 397: 0.190756\n",
      "Cost after iteration 398: 0.190521\n",
      "Cost after iteration 399: 0.190287\n",
      "Cost after iteration 400: 0.190053\n",
      "Cost after iteration 401: 0.189821\n",
      "Cost after iteration 402: 0.189589\n",
      "Cost after iteration 403: 0.189358\n",
      "Cost after iteration 404: 0.189128\n",
      "Cost after iteration 405: 0.188898\n",
      "Cost after iteration 406: 0.188670\n",
      "Cost after iteration 407: 0.188442\n",
      "Cost after iteration 408: 0.188215\n",
      "Cost after iteration 409: 0.187989\n",
      "Cost after iteration 410: 0.187764\n",
      "Cost after iteration 411: 0.187539\n",
      "Cost after iteration 412: 0.187316\n",
      "Cost after iteration 413: 0.187093\n",
      "Cost after iteration 414: 0.186870\n",
      "Cost after iteration 415: 0.186649\n",
      "Cost after iteration 416: 0.186428\n",
      "Cost after iteration 417: 0.186209\n",
      "Cost after iteration 418: 0.185989\n",
      "Cost after iteration 419: 0.185771\n",
      "Cost after iteration 420: 0.185553\n",
      "Cost after iteration 421: 0.185337\n",
      "Cost after iteration 422: 0.185120\n",
      "Cost after iteration 423: 0.184905\n",
      "Cost after iteration 424: 0.184690\n",
      "Cost after iteration 425: 0.184476\n",
      "Cost after iteration 426: 0.184263\n",
      "Cost after iteration 427: 0.184051\n",
      "Cost after iteration 428: 0.183839\n",
      "Cost after iteration 429: 0.183628\n",
      "Cost after iteration 430: 0.183417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 431: 0.183208\n",
      "Cost after iteration 432: 0.182999\n",
      "Cost after iteration 433: 0.182790\n",
      "Cost after iteration 434: 0.182583\n",
      "Cost after iteration 435: 0.182376\n",
      "Cost after iteration 436: 0.182170\n",
      "Cost after iteration 437: 0.181964\n",
      "Cost after iteration 438: 0.181759\n",
      "Cost after iteration 439: 0.181555\n",
      "Cost after iteration 440: 0.181352\n",
      "Cost after iteration 441: 0.181149\n",
      "Cost after iteration 442: 0.180947\n",
      "Cost after iteration 443: 0.180745\n",
      "Cost after iteration 444: 0.180544\n",
      "Cost after iteration 445: 0.180344\n",
      "Cost after iteration 446: 0.180144\n",
      "Cost after iteration 447: 0.179945\n",
      "Cost after iteration 448: 0.179747\n",
      "Cost after iteration 449: 0.179549\n",
      "Cost after iteration 450: 0.179352\n",
      "Cost after iteration 451: 0.179156\n",
      "Cost after iteration 452: 0.178960\n",
      "Cost after iteration 453: 0.178765\n",
      "Cost after iteration 454: 0.178570\n",
      "Cost after iteration 455: 0.178377\n",
      "Cost after iteration 456: 0.178183\n",
      "Cost after iteration 457: 0.177991\n",
      "Cost after iteration 458: 0.177798\n",
      "Cost after iteration 459: 0.177607\n",
      "Cost after iteration 460: 0.177416\n",
      "Cost after iteration 461: 0.177226\n",
      "Cost after iteration 462: 0.177036\n",
      "Cost after iteration 463: 0.176847\n",
      "Cost after iteration 464: 0.176658\n",
      "Cost after iteration 465: 0.176470\n",
      "Cost after iteration 466: 0.176283\n",
      "Cost after iteration 467: 0.176096\n",
      "Cost after iteration 468: 0.175910\n",
      "Cost after iteration 469: 0.175724\n",
      "Cost after iteration 470: 0.175539\n",
      "Cost after iteration 471: 0.175355\n",
      "Cost after iteration 472: 0.175171\n",
      "Cost after iteration 473: 0.174987\n",
      "Cost after iteration 474: 0.174805\n",
      "Cost after iteration 475: 0.174622\n",
      "Cost after iteration 476: 0.174441\n",
      "Cost after iteration 477: 0.174259\n",
      "Cost after iteration 478: 0.174079\n",
      "Cost after iteration 479: 0.173899\n",
      "Cost after iteration 480: 0.173719\n",
      "Cost after iteration 481: 0.173540\n",
      "Cost after iteration 482: 0.173362\n",
      "Cost after iteration 483: 0.173184\n",
      "Cost after iteration 484: 0.173006\n",
      "Cost after iteration 485: 0.172830\n",
      "Cost after iteration 486: 0.172653\n",
      "Cost after iteration 487: 0.172477\n",
      "Cost after iteration 488: 0.172302\n",
      "Cost after iteration 489: 0.172127\n",
      "Cost after iteration 490: 0.171953\n",
      "Cost after iteration 491: 0.171779\n",
      "Cost after iteration 492: 0.171606\n",
      "Cost after iteration 493: 0.171433\n",
      "Cost after iteration 494: 0.171261\n",
      "Cost after iteration 495: 0.171089\n",
      "Cost after iteration 496: 0.170918\n",
      "Cost after iteration 497: 0.170747\n",
      "Cost after iteration 498: 0.170577\n",
      "Cost after iteration 499: 0.170407\n",
      "[[-0.05291107]]\n",
      "train accuracy: 95.79439252336448 %\n",
      "test accuracy: 95.02074688796681 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 500, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
