\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\title{Logistic and Softmax Regression for Handwritten Digits Classification}


\author{
Shilin Zhu \\
Ph.D. student, Computer Science\\
UCSD\\
La Jolla, CA \\
\texttt{shz338@eng.ucsd.edu} \\
\And
Yunhui Guo\\
Ph.D. student, Computer Science\\
UCSD\\
La Jolla, CA \\
\texttt{email} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\section{Logistic Regression via Gradient Descent}

\subsection{Problem Definition}
In this work, we realize the handwritten digit classification using MNIST database. Our goal is to accurately and robustly recognize what number is present in a new image. In this section we will first use logistic regression to classify only two digit classes (binary classification), later in the next section we will use softmax regression to generalize logistic regression into $N$ classes.

\subsection{Mathematical Derivation}
\textbf{Derive the gradient for logistic regression:} \\
The cross-entropy cost function can be expressed as

\begin{equation}
	E(w) = - \sum_{n=1}^{N}[t^{n}\ln(y^{n})+(1-t^{n})\ln(1-y^{n})]
\end{equation}

where $t^{n}$ is the target label for example $n$ and $y^{n}$ is our prediction for this example. To perform gradient descent, we need to first compute the gradient (derivative) of the cost function with respect to the parameters. The gradient of cost function on example $n$ is

\begin{equation}
	-\frac{\partial E^{n}(w)}{\partial w_{j}} = \frac{\partial [t^{n}\ln(y^{n})+(1-t^{n})\ln(1-y^{n})]}{\partial w_{j}} 
\end{equation}

where $y^{n}$ is the prediction of logistic regression as

\begin{equation}
	y^{n} = g(\sum_{j=0}^{m}w_{j}x_{j}^{n})
\end{equation}

and $g(\cdot)$ is the sigmoid activation function and its derivative is

\begin{equation}
	g^{\prime}(z^{n}) = \frac{d(\frac{1}{1+e^{-z^{n}}})}{dz^{n}} = g(z^{n})(1-g(z^{n}))
\end{equation}

where $z ^{n}= \sum_{j=0}^{m}w_{j}x_{j}^{n}$. According to the chain rule in calculus, we can then compute the gradient (derivative) of the cost function on example $n$ with respect to the parameters as

\begin{equation}
	-\frac{\partial E^{n}(w)}{\partial w_{j}} = -\frac{\partial E^{n}(w)}{\partial y^{n}}\frac{\partial y^{n}}{\partial z^{n}}\frac{\partial z^{n}}{\partial w_{j}} = (\frac{t^{n}}{y^{n}}-\frac{1-t^{n}}{1-y^{n}})\cdot y^{n}(1-y^{n})\cdot x_{j}^{n} = (t^{n}-y^{n})x_{j}^{n}
\end{equation}

\subsection{Data Reading, Parsing and Feature Extraction} \\
In this work we use the famous MNIST hand written digits database created by Yann LeCun. Each image in the database contains $28 \times 28$ pixels and each pixel has a grayscale intensity, so that the input feature $x \in R^{784}$ after we unroll the 2D image into an 1D vector. To include the bias term, after reading the data, we append a '1' to the beginning of each $x$ vector so the final $x \in R^{785}$. We will use the first 20,000 training images and the last 2,000 test images. Note that for logistic regression to do binary classification, we can only use the images of two specific digit classes, so that the actual training images and test images are smaller than 20,000 and 2,000 respectively. To speed up learning, we need to apply feature normalization. For images, the most common way is to normalize the pixel values by the maximum pixel value 255. After normalization, all the values in $x$ is now ranging from 0 to 1.

The following code shows how we can choose the example images corresponding to two specific digit classes. For the rest of the report, we choose two binary classification problems: 2 vs. 3 and 2 vs. 8.

\textit{TODO: Attach code here.}

\subsection{Experimental Results}
\textbf{Batch gradient descent}\\
We first apply batch gradient descent rule on logistic regression since the training set is not that huge, thus batch gradient descent can work reasonably fast. 

\begin{figure}%
	\centering
	\subfloat[cost function with iterations for batch GD on classifying 2 vs. 3]{{\includegraphics[width=2.5in]{images/P1_2.png}  }}%
	\qquad
	\subfloat[cost function with iterations for batch GD on classifying 2 vs. 8]{{	\includegraphics[width=2.5in]{images/P1.png}  }}%
	\caption{Cost function through batch GD training process}%
	\label{fig:P1}%
\end{figure}

\begin{figure}%
	\centering
	\subfloat[cost function with iterations for mini-batch GD on classifying 2 vs. 3]{{\includegraphics[width=2.5in]{images/P2_2.png}  }}%
	\qquad
	\subfloat[cost function with iterations for mini-batch GD on classifying 2 vs. 8]{{	\includegraphics[width=2.5in]{images/P2.png}  }}%
	\caption{Cost function through mini-batch GD training process}%
	\label{fig:P2}%
\end{figure}

After trying different values of learning rate, we found $0.005$ is a reasonably good learning rate for this problem. Fig. \ref{fig: P1}(a) plots the  loss function over training for the training set, the hold-out validation set and the test set. From the results we can see the model tries to minimize the cost and meanwhile maximize the likelihood to perform good prediction on the data.

\textbf{Mini-Batch gradient descent}\\
We can use mini-batch gradient descent to speed up learning process. Since the one-step optimization is done on a smaller mini-batch, the cost function will not monotonically decrease as batch gradient descent. Here we change the weights after a mini-batch of roughly $10\%$ of the entire training set. This set of mini-batch size can result in relatively smooth curve of the cost function. Fig. \ref{fig: P1}(b) plots the loss function over training for consecutive mini-batches. Since the validation set always has very similar errors as the test set, we can conclude the hold-out set work as a good stand-in for the test set and their underlying data distributions are same.

\textbf{Accuracy of Prediction Using Logistic Regression Classifier}\\

\begin{figure}%
	\centering
	\subfloat[accuracy with iterations on classifying 2 vs. 3]{{\includegraphics[width=2.5in]{images/P3_2.png}  }}%
	\qquad
	\subfloat[accuracy with iterations on classifying 2 vs. 8]{{	\includegraphics[width=2.5in]{images/P3.png}  }}%
	\caption{Accuracy through training process}%
	\label{fig:P3}%
\end{figure}

\textbf{Weight Visualization}\\
We can visualize the weights learned to see what logistic regression learns through the training process. 

\begin{figure}%
	\centering
	\subfloat[weights learned on classifying 2 vs. 3]{{\includegraphics[width=2.5in]{images/P4_2.png}  }}%
	\qquad
	\subfloat[weights learned on classifying 2 vs. 8]{{	\includegraphics[width=2.5in]{images/P4.png}  }}%
	\caption{Weight visualization on binary classification using logistic regression}%
	\label{fig:P4}%
\end{figure}

Computing the difference between these two different classifiers, we can get a new weight matrix visualized in Fig. \ref{fig: P5}. This result shows that the new weights can be used to accurately classify digit 3 and digit 8.

\begin{figure}[t]
 \centering
 \includegraphics[width=0.4\linewidth]{images/P5.png}
 \caption{Difference of the weights between the two classifiers (2 vs. 3 and 2 vs. 8)}
 \label{fig:P5}
\end{figure}

\textbf{Derive the gradient for regularized logistic regression:} \\

To prevent potential overfitting, regularization is used in logistic regression. The cross-entropy cost function with regularization term can be computed as

\begin{equation}
	E(w) = - \sum_{n=1}^{N}[t^{n}\ln(y^{n})+(1-t^{n})\ln(1-y^{n})] + \lambda * C(w)
\end{equation}

where $C(w)$ represents the complexity of the model. $L1$ and $L2$ regularizations are two most common functions people use

\begin{equation}
	C(w) = ||w||^{2} = \sum_{i, j}w_{i, j}^{2}(L2)
\end{equation}

\begin{equation}
	C(w) = |w| = \sum_{i, j}|w_{i, j}|(L1)
\end{equation}

Thus the gradient of cost function on example $n$ is 

\[
    -\frac{\partial E^{n}(w)}{\partial w_{j}} = 
\begin{cases}
    (t^{n}-y^{n})x_{j}^{n} - 2\lambda w_{j},& \text{if } L2\\
    (t^{n}-y^{n})x_{j}^{n} - \lambda sign(w_{j}), & \text{if } L1
\end{cases}
\]

where $sign(w_{j})$ is the signature function of $w_{j}$. 

Note that we can always add a factor of $1/N$ to scale the cost and gradient to somehow speed up the learning, and this will not change the optimization results (learned parameters).


\section{Softmax Regression via Gradient Descent}
\subsection{Problem definition}
In this problem, we need to classify MNIST datasets using softmax regression. In the experiments, we only use the first 20,000 training images and the last 2,000 test images.

\subsection{Methods}
We use softmax regresion for this problem. \\
\textbf{Derive the gradient for Softmax Regression:} \\
The cross-entropy cost function can be expressed as,

\begin{equation}
	E = - \sum_{n}\sum_{k=1}^{c}t_k^n\ln y_k^n
\end{equation}

Where, 
\begin{equation}
	y_k^n = \frac{\exp{(a_k^n)}}{ \sum_{k'}\exp{(a_{k'}^n})}
\end{equation}
And,
\begin{equation}
	a_k^n = w_k^Tx^n
\end{equation}
We can calculate the gradient for softmax regression as follows,
\begin{equation}
\begin{split}
-\frac{\partial E^n(w)}{\partial w_{jk}}& = - \frac{\partial E^n(w)}{\partial a_k^n}\frac{\partial a_k^n}{\partial w_{jk}} \\
& = -\sum_{k'} \frac{\partial E^n(w)}{\partial y^n_{k'}} \frac{\partial y^n_{k'}}{\partial a^n_k}\frac{\partial a_k^n}{\partial w_{jk}} \\
\end{split}
\end{equation}
And 
\begin{equation}
\begin{split}
\frac{\partial y^n_{k'}}{\partial a^n_k} =  y^n_{k'}\delta_{kk'} - y_{k
'}y_{k}\\
\end{split}
\end{equation}
Where $\delta_{kk} = 1$ if $k = k'$, otherwise $\delta_{kk} = 0$.
And
\begin{equation}
\begin{split}
	\frac{\partial E^n(w)}{\partial y^n_{k'}} = -\frac{t_{k'}}{y_{k'}}
\end{split}
\end{equation}
Substitute Equation (5) and Equation (6) into Equation (4) we get,
\begin{equation}
\begin{split}
	-\frac{\partial E^n(w)}{\partial w_{jk}} = (t_k - y_k)x_j^n 
\end{split}
\end{equation}


\textbf{Preprossing}: First, we extract the first 20,000 training images and the last 2,000 test images. Then normailize the images to make sure the pixel values are in the range of [0,1]. Convert the labels to one-hot vectors. Divide the training images into two parts, the first 10\% are used for as a hold-out set and the rest 90\% are used for training.
\\
\textbf{Experiments settings:} To determine the best type of regurization and the best $\lambda$, we try $L_2$ regularization and $L_1$ regularization seperately.\\
 For the $L_2$ regularizartion, we search the best $\lambda$ in the set $\{0.01, 0.001, 0.0001\}$. If the accuracy on the hold-out set decreases for 3 epochs, we stop the algorithm and use the weights with the minimum error (highest accuracy) on the hold-out set as the final answer. For the $L_1$ regularization, we follow the same steps. Then we compare the results get from these two regularization methods and use it as the best final result.
 
\subsection{Results}
(a) In the experiments, we find that using $L_2$ regularization with $\lambda = 0.01$ obtain the best result on the validation set. With an accuracy of 0.9045\% on the validation set. With such settings, the accuracy on the test set is 0.927\%.


\begin{figure}%
	\centering
	\subfloat[label 1]{{\includegraphics[width=2.5in]{../softmax/figs/accuracy.png}  }}%
	\qquad
	\subfloat[label 2]{{	\includegraphics[width=2.5in]{../softmax/figs/loss.png}  }}%
	\caption{2 Figures side by side}%
	\label{fig:example}%
\end{figure}



(b) In this experiement, we use  $L_2$ regularization with $\lambda = 0.01$. The figure is shown in Fig \ref{figure: 1}. 


(c) In this experiement, we use  $L_2$ regularization with $\lambda = 0.01$. The figure is shown in Fig \ref{figure: 2}.

(d) We plot the results in Fig \ref{figure: 3}. We can see that the image of the weight and the corresponding image of the average digit is almost the same. The reason is that we classify the images based on the inner product of the pixesls with the weights. And the inner product is maximized when the angle between the weight and the image is zero. So we see that the image of the weight and the corresponding image of the average digit is similar.

\begin{figure}[h]
	\centering	
	\includegraphics[width=2.8in,height=3.2in]{../softmax/figs/image_of_weights_and_digits.png} 
	\caption{}
	\label{figure: 3}
\end{figure}

\subsection{Discussion}




\subsubsection*{Acknowledgments}

. 

\subsubsection*{References}

\end{document}
