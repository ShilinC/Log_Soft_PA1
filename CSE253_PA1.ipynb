{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic and Softmax Regression for Handwritten Digits Classification \n",
    "Programming Assignment 1\n",
    "CSE 253: Neural Networks for Pattern Recognition, by Prof. Gary Cottrell\n",
    "\n",
    "Shilin Zhu\n",
    "Ph.D. student, Computer Science, UCSD\n",
    "shz338@eng.ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "Run the code below to import the packages we need in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MNIST\n",
    "MNIST is a handwritten digit database by Yann LeCun. To reduce the computation, we only use 20000 images for training and 2000 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d7b92b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10898cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('./python-mnist/data')\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "plt.imshow(np.array(train_images[0]).reshape([28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We only use 20000 images for training and 2000 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "test_images = np.array(test_images)\n",
    "train_labels = np.array(train_labels).reshape([60000,1])\n",
    "test_labels = np.array(test_labels).reshape([10000,1])\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1)\n"
     ]
    }
   ],
   "source": [
    "print((train_images[1,:]).reshape(784,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to only retain the examples with two labels to perform binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "test_size = 1000\n",
    "train_images_sub = np.zeros([784,train_size])\n",
    "train_labels_sub = np.zeros([1,train_size])\n",
    "test_images_sub = np.zeros([784,test_size])\n",
    "test_labels_sub = np.zeros([1,test_size])\n",
    "counter = 0\n",
    "for i in range(0,train_size):\n",
    "    if train_labels[i][0] == 1 or train_labels[i][0] == 2:\n",
    "        train_images_sub[:,counter] = train_images[i,:]\n",
    "        train_labels_sub[:,counter] = train_labels[i,:]-1\n",
    "        counter = counter + 1        \n",
    "train_images_sub = train_images_sub[:,0:counter-1]\n",
    "train_labels_sub = train_labels_sub[:,0:counter-1]\n",
    "\n",
    "counter = 0\n",
    "for i in range(0,test_size):\n",
    "    if test_labels[i][0] == 1 or test_labels[i][0] == 2:\n",
    "        test_images_sub[:,counter] = test_images[i,:]\n",
    "        test_labels_sub[:,counter] = test_labels[i,:]-1\n",
    "        counter = counter + 1        \n",
    "test_images_sub = test_images_sub[:,0:counter-1]\n",
    "test_labels_sub = test_labels_sub[:,0:counter-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 214)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10b110a90>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d82e9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_images_sub.shape)\n",
    "plt.imshow(np.array(train_images_sub[:,5]).reshape([28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize the features (already flattened) by dividing over 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_images_sub/255.\n",
    "test_set_x = test_images_sub/255.\n",
    "train_set_y = train_labels_sub\n",
    "test_set_y = test_labels_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.48627451  0.99215686\n",
      "  1.          0.24705882  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.37647059  0.95686275\n",
      "  0.98431373  0.99215686  0.24313725  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.49803922\n",
      "  0.98431373  0.98431373  0.99215686  0.24313725  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.26666667\n",
      "  0.9254902   0.98431373  0.82745098  0.12156863  0.03137255  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.23529412\n",
      "  0.89411765  0.98431373  0.98431373  0.36862745  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.60784314  0.99215686  0.99215686  0.74117647  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.07843137  0.99215686  0.98431373  0.92156863  0.25882353  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.1254902\n",
      "  0.80392157  0.99215686  0.98431373  0.49411765  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.40784314  0.98431373  0.99215686  0.72156863  0.05882353  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.31372549  0.94117647  0.98431373  0.75686275  0.09019608  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.1254902\n",
      "  0.99215686  0.99215686  0.99215686  0.62352941  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.59215686  0.98431373  0.98431373  0.98431373  0.15294118  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.18823529  0.86666667  0.98431373  0.98431373  0.6745098   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.91764706  0.98431373  0.98431373  0.76862745  0.04705882  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.99215686  0.98431373  0.98431373  0.34901961  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.62352941  1.          0.99215686  0.99215686  0.12156863  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.18823529  0.89411765  0.99215686  0.96862745  0.54901961  0.03137255\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.25098039  0.98431373  0.99215686  0.8627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.25098039  0.98431373  0.99215686  0.8627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.09411765  0.75686275  0.99215686  0.8627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_set_x[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we perform the following steps:\n",
    "-initialize the parameters\n",
    "-learn the parameters using logistic/softmax regression\n",
    "-use the learned parameters to make predictions\n",
    "-analyze and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build some helper functions so that we will build the entire model later based on these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    a = 1/(1+np.exp(-z))  \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic/softmax regression, zero initialization can work reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros([dim,1])\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a simple one-layer neural network, so we reuse the term forward propagation and backward propagation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    cost = -(1/m)*np.sum(np.multiply(Y,np.log(A))+np.multiply(1-Y,np.log(1-A)),keepdims=True)                                 # compute cost\n",
    "\n",
    "    dw = (1/m)*(np.dot(X,(A-Y).T))\n",
    "    \n",
    "    db = (1/m)*np.sum(A-Y,keepdims=True)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we compute the gradients, we can use gradient descent to optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the learned parameters to predict labels for the unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        Y_prediction[0,i] = np.where(A[0,i]>0.5,1,0)\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Entire Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our implemented helper functions, we can now build the entire model by combining them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = True):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    w, b = np.zeros((X_train.shape[0],1)), 0.\n",
    "    \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    print(b)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now use the following code to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 1: 0.679062\n",
      "Cost after iteration 2: 0.665589\n",
      "Cost after iteration 3: 0.652688\n",
      "Cost after iteration 4: 0.640324\n",
      "Cost after iteration 5: 0.628463\n",
      "Cost after iteration 6: 0.617077\n",
      "Cost after iteration 7: 0.606136\n",
      "Cost after iteration 8: 0.595617\n",
      "Cost after iteration 9: 0.585496\n",
      "Cost after iteration 10: 0.575752\n",
      "Cost after iteration 11: 0.566365\n",
      "Cost after iteration 12: 0.557318\n",
      "Cost after iteration 13: 0.548593\n",
      "Cost after iteration 14: 0.540175\n",
      "Cost after iteration 15: 0.532048\n",
      "Cost after iteration 16: 0.524199\n",
      "Cost after iteration 17: 0.516616\n",
      "Cost after iteration 18: 0.509285\n",
      "Cost after iteration 19: 0.502195\n",
      "Cost after iteration 20: 0.495335\n",
      "Cost after iteration 21: 0.488696\n",
      "Cost after iteration 22: 0.482267\n",
      "Cost after iteration 23: 0.476040\n",
      "Cost after iteration 24: 0.470005\n",
      "Cost after iteration 25: 0.464154\n",
      "Cost after iteration 26: 0.458480\n",
      "Cost after iteration 27: 0.452975\n",
      "Cost after iteration 28: 0.447632\n",
      "Cost after iteration 29: 0.442445\n",
      "Cost after iteration 30: 0.437406\n",
      "Cost after iteration 31: 0.432511\n",
      "Cost after iteration 32: 0.427753\n",
      "Cost after iteration 33: 0.423127\n",
      "Cost after iteration 34: 0.418627\n",
      "Cost after iteration 35: 0.414249\n",
      "Cost after iteration 36: 0.409988\n",
      "Cost after iteration 37: 0.405840\n",
      "Cost after iteration 38: 0.401800\n",
      "Cost after iteration 39: 0.397864\n",
      "Cost after iteration 40: 0.394028\n",
      "Cost after iteration 41: 0.390288\n",
      "Cost after iteration 42: 0.386642\n",
      "Cost after iteration 43: 0.383085\n",
      "Cost after iteration 44: 0.379614\n",
      "Cost after iteration 45: 0.376227\n",
      "Cost after iteration 46: 0.372920\n",
      "Cost after iteration 47: 0.369690\n",
      "Cost after iteration 48: 0.366535\n",
      "Cost after iteration 49: 0.363453\n",
      "Cost after iteration 50: 0.360440\n",
      "Cost after iteration 51: 0.357495\n",
      "Cost after iteration 52: 0.354614\n",
      "Cost after iteration 53: 0.351797\n",
      "Cost after iteration 54: 0.349041\n",
      "Cost after iteration 55: 0.346343\n",
      "Cost after iteration 56: 0.343703\n",
      "Cost after iteration 57: 0.341117\n",
      "Cost after iteration 58: 0.338585\n",
      "Cost after iteration 59: 0.336105\n",
      "Cost after iteration 60: 0.333675\n",
      "Cost after iteration 61: 0.331294\n",
      "Cost after iteration 62: 0.328959\n",
      "Cost after iteration 63: 0.326671\n",
      "Cost after iteration 64: 0.324426\n",
      "Cost after iteration 65: 0.322225\n",
      "Cost after iteration 66: 0.320065\n",
      "Cost after iteration 67: 0.317946\n",
      "Cost after iteration 68: 0.315866\n",
      "Cost after iteration 69: 0.313824\n",
      "Cost after iteration 70: 0.311820\n",
      "Cost after iteration 71: 0.309851\n",
      "Cost after iteration 72: 0.307918\n",
      "Cost after iteration 73: 0.306018\n",
      "Cost after iteration 74: 0.304152\n",
      "Cost after iteration 75: 0.302318\n",
      "Cost after iteration 76: 0.300515\n",
      "Cost after iteration 77: 0.298743\n",
      "Cost after iteration 78: 0.297001\n",
      "Cost after iteration 79: 0.295287\n",
      "Cost after iteration 80: 0.293602\n",
      "Cost after iteration 81: 0.291944\n",
      "Cost after iteration 82: 0.290313\n",
      "Cost after iteration 83: 0.288708\n",
      "Cost after iteration 84: 0.287128\n",
      "Cost after iteration 85: 0.285573\n",
      "Cost after iteration 86: 0.284042\n",
      "Cost after iteration 87: 0.282535\n",
      "Cost after iteration 88: 0.281051\n",
      "Cost after iteration 89: 0.279589\n",
      "Cost after iteration 90: 0.278149\n",
      "Cost after iteration 91: 0.276730\n",
      "Cost after iteration 92: 0.275333\n",
      "Cost after iteration 93: 0.273955\n",
      "Cost after iteration 94: 0.272598\n",
      "Cost after iteration 95: 0.271260\n",
      "Cost after iteration 96: 0.269941\n",
      "Cost after iteration 97: 0.268640\n",
      "Cost after iteration 98: 0.267358\n",
      "Cost after iteration 99: 0.266093\n",
      "Cost after iteration 100: 0.264846\n",
      "Cost after iteration 101: 0.263616\n",
      "Cost after iteration 102: 0.262402\n",
      "Cost after iteration 103: 0.261204\n",
      "Cost after iteration 104: 0.260023\n",
      "Cost after iteration 105: 0.258857\n",
      "Cost after iteration 106: 0.257706\n",
      "Cost after iteration 107: 0.256570\n",
      "Cost after iteration 108: 0.255448\n",
      "Cost after iteration 109: 0.254341\n",
      "Cost after iteration 110: 0.253248\n",
      "Cost after iteration 111: 0.252168\n",
      "Cost after iteration 112: 0.251102\n",
      "Cost after iteration 113: 0.250049\n",
      "Cost after iteration 114: 0.249009\n",
      "Cost after iteration 115: 0.247982\n",
      "Cost after iteration 116: 0.246967\n",
      "Cost after iteration 117: 0.245964\n",
      "Cost after iteration 118: 0.244973\n",
      "Cost after iteration 119: 0.243993\n",
      "Cost after iteration 120: 0.243026\n",
      "Cost after iteration 121: 0.242069\n",
      "Cost after iteration 122: 0.241123\n",
      "Cost after iteration 123: 0.240188\n",
      "Cost after iteration 124: 0.239264\n",
      "Cost after iteration 125: 0.238350\n",
      "Cost after iteration 126: 0.237447\n",
      "Cost after iteration 127: 0.236553\n",
      "Cost after iteration 128: 0.235669\n",
      "Cost after iteration 129: 0.234795\n",
      "Cost after iteration 130: 0.233930\n",
      "Cost after iteration 131: 0.233075\n",
      "Cost after iteration 132: 0.232229\n",
      "Cost after iteration 133: 0.231392\n",
      "Cost after iteration 134: 0.230564\n",
      "Cost after iteration 135: 0.229744\n",
      "Cost after iteration 136: 0.228933\n",
      "Cost after iteration 137: 0.228130\n",
      "Cost after iteration 138: 0.227336\n",
      "Cost after iteration 139: 0.226550\n",
      "Cost after iteration 140: 0.225771\n",
      "Cost after iteration 141: 0.225001\n",
      "Cost after iteration 142: 0.224238\n",
      "Cost after iteration 143: 0.223483\n",
      "Cost after iteration 144: 0.222735\n",
      "Cost after iteration 145: 0.221995\n",
      "Cost after iteration 146: 0.221261\n",
      "Cost after iteration 147: 0.220535\n",
      "Cost after iteration 148: 0.219816\n",
      "Cost after iteration 149: 0.219104\n",
      "Cost after iteration 150: 0.218399\n",
      "Cost after iteration 151: 0.217700\n",
      "Cost after iteration 152: 0.217008\n",
      "Cost after iteration 153: 0.216322\n",
      "Cost after iteration 154: 0.215642\n",
      "Cost after iteration 155: 0.214969\n",
      "Cost after iteration 156: 0.214302\n",
      "Cost after iteration 157: 0.213641\n",
      "Cost after iteration 158: 0.212986\n",
      "Cost after iteration 159: 0.212337\n",
      "Cost after iteration 160: 0.211694\n",
      "Cost after iteration 161: 0.211057\n",
      "Cost after iteration 162: 0.210425\n",
      "Cost after iteration 163: 0.209798\n",
      "Cost after iteration 164: 0.209177\n",
      "Cost after iteration 165: 0.208562\n",
      "Cost after iteration 166: 0.207952\n",
      "Cost after iteration 167: 0.207347\n",
      "Cost after iteration 168: 0.206747\n",
      "Cost after iteration 169: 0.206152\n",
      "Cost after iteration 170: 0.205562\n",
      "Cost after iteration 171: 0.204978\n",
      "Cost after iteration 172: 0.204398\n",
      "Cost after iteration 173: 0.203823\n",
      "Cost after iteration 174: 0.203252\n",
      "Cost after iteration 175: 0.202687\n",
      "Cost after iteration 176: 0.202126\n",
      "Cost after iteration 177: 0.201569\n",
      "Cost after iteration 178: 0.201017\n",
      "Cost after iteration 179: 0.200469\n",
      "Cost after iteration 180: 0.199926\n",
      "Cost after iteration 181: 0.199387\n",
      "Cost after iteration 182: 0.198852\n",
      "Cost after iteration 183: 0.198322\n",
      "Cost after iteration 184: 0.197796\n",
      "Cost after iteration 185: 0.197273\n",
      "Cost after iteration 186: 0.196755\n",
      "Cost after iteration 187: 0.196241\n",
      "Cost after iteration 188: 0.195731\n",
      "Cost after iteration 189: 0.195224\n",
      "Cost after iteration 190: 0.194722\n",
      "Cost after iteration 191: 0.194223\n",
      "Cost after iteration 192: 0.193728\n",
      "Cost after iteration 193: 0.193236\n",
      "Cost after iteration 194: 0.192748\n",
      "Cost after iteration 195: 0.192264\n",
      "Cost after iteration 196: 0.191784\n",
      "Cost after iteration 197: 0.191307\n",
      "Cost after iteration 198: 0.190833\n",
      "Cost after iteration 199: 0.190363\n",
      "Cost after iteration 200: 0.189896\n",
      "Cost after iteration 201: 0.189432\n",
      "Cost after iteration 202: 0.188972\n",
      "Cost after iteration 203: 0.188515\n",
      "Cost after iteration 204: 0.188061\n",
      "Cost after iteration 205: 0.187611\n",
      "Cost after iteration 206: 0.187164\n",
      "Cost after iteration 207: 0.186719\n",
      "Cost after iteration 208: 0.186278\n",
      "Cost after iteration 209: 0.185840\n",
      "Cost after iteration 210: 0.185405\n",
      "Cost after iteration 211: 0.184973\n",
      "Cost after iteration 212: 0.184543\n",
      "Cost after iteration 213: 0.184117\n",
      "Cost after iteration 214: 0.183693\n",
      "Cost after iteration 215: 0.183273\n",
      "Cost after iteration 216: 0.182855\n",
      "Cost after iteration 217: 0.182440\n",
      "Cost after iteration 218: 0.182027\n",
      "Cost after iteration 219: 0.181618\n",
      "Cost after iteration 220: 0.181211\n",
      "Cost after iteration 221: 0.180807\n",
      "Cost after iteration 222: 0.180405\n",
      "Cost after iteration 223: 0.180006\n",
      "Cost after iteration 224: 0.179609\n",
      "Cost after iteration 225: 0.179215\n",
      "Cost after iteration 226: 0.178824\n",
      "Cost after iteration 227: 0.178435\n",
      "Cost after iteration 228: 0.178048\n",
      "Cost after iteration 229: 0.177664\n",
      "Cost after iteration 230: 0.177282\n",
      "Cost after iteration 231: 0.176903\n",
      "Cost after iteration 232: 0.176526\n",
      "Cost after iteration 233: 0.176152\n",
      "Cost after iteration 234: 0.175779\n",
      "Cost after iteration 235: 0.175409\n",
      "Cost after iteration 236: 0.175041\n",
      "Cost after iteration 237: 0.174676\n",
      "Cost after iteration 238: 0.174312\n",
      "Cost after iteration 239: 0.173951\n",
      "Cost after iteration 240: 0.173592\n",
      "Cost after iteration 241: 0.173235\n",
      "Cost after iteration 242: 0.172881\n",
      "Cost after iteration 243: 0.172528\n",
      "Cost after iteration 244: 0.172178\n",
      "Cost after iteration 245: 0.171829\n",
      "Cost after iteration 246: 0.171483\n",
      "Cost after iteration 247: 0.171138\n",
      "Cost after iteration 248: 0.170796\n",
      "Cost after iteration 249: 0.170455\n",
      "Cost after iteration 250: 0.170117\n",
      "Cost after iteration 251: 0.169780\n",
      "Cost after iteration 252: 0.169446\n",
      "Cost after iteration 253: 0.169113\n",
      "Cost after iteration 254: 0.168782\n",
      "Cost after iteration 255: 0.168453\n",
      "Cost after iteration 256: 0.168126\n",
      "Cost after iteration 257: 0.167801\n",
      "Cost after iteration 258: 0.167477\n",
      "Cost after iteration 259: 0.167156\n",
      "Cost after iteration 260: 0.166836\n",
      "Cost after iteration 261: 0.166518\n",
      "Cost after iteration 262: 0.166201\n",
      "Cost after iteration 263: 0.165887\n",
      "Cost after iteration 264: 0.165574\n",
      "Cost after iteration 265: 0.165263\n",
      "Cost after iteration 266: 0.164953\n",
      "Cost after iteration 267: 0.164645\n",
      "Cost after iteration 268: 0.164339\n",
      "Cost after iteration 269: 0.164034\n",
      "Cost after iteration 270: 0.163731\n",
      "Cost after iteration 271: 0.163430\n",
      "Cost after iteration 272: 0.163130\n",
      "Cost after iteration 273: 0.162832\n",
      "Cost after iteration 274: 0.162535\n",
      "Cost after iteration 275: 0.162240\n",
      "Cost after iteration 276: 0.161947\n",
      "Cost after iteration 277: 0.161655\n",
      "Cost after iteration 278: 0.161364\n",
      "Cost after iteration 279: 0.161075\n",
      "Cost after iteration 280: 0.160788\n",
      "Cost after iteration 281: 0.160502\n",
      "Cost after iteration 282: 0.160217\n",
      "Cost after iteration 283: 0.159934\n",
      "Cost after iteration 284: 0.159652\n",
      "Cost after iteration 285: 0.159372\n",
      "Cost after iteration 286: 0.159093\n",
      "Cost after iteration 287: 0.158816\n",
      "Cost after iteration 288: 0.158540\n",
      "Cost after iteration 289: 0.158265\n",
      "Cost after iteration 290: 0.157992\n",
      "Cost after iteration 291: 0.157720\n",
      "Cost after iteration 292: 0.157449\n",
      "Cost after iteration 293: 0.157180\n",
      "Cost after iteration 294: 0.156912\n",
      "Cost after iteration 295: 0.156645\n",
      "Cost after iteration 296: 0.156380\n",
      "Cost after iteration 297: 0.156116\n",
      "Cost after iteration 298: 0.155853\n",
      "Cost after iteration 299: 0.155591\n",
      "Cost after iteration 300: 0.155331\n",
      "Cost after iteration 301: 0.155072\n",
      "Cost after iteration 302: 0.154814\n",
      "Cost after iteration 303: 0.154557\n",
      "Cost after iteration 304: 0.154302\n",
      "Cost after iteration 305: 0.154048\n",
      "Cost after iteration 306: 0.153795\n",
      "Cost after iteration 307: 0.153543\n",
      "Cost after iteration 308: 0.153292\n",
      "Cost after iteration 309: 0.153043\n",
      "Cost after iteration 310: 0.152794\n",
      "Cost after iteration 311: 0.152547\n",
      "Cost after iteration 312: 0.152301\n",
      "Cost after iteration 313: 0.152056\n",
      "Cost after iteration 314: 0.151813\n",
      "Cost after iteration 315: 0.151570\n",
      "Cost after iteration 316: 0.151328\n",
      "Cost after iteration 317: 0.151088\n",
      "Cost after iteration 318: 0.150848\n",
      "Cost after iteration 319: 0.150610\n",
      "Cost after iteration 320: 0.150373\n",
      "Cost after iteration 321: 0.150137\n",
      "Cost after iteration 322: 0.149902\n",
      "Cost after iteration 323: 0.149668\n",
      "Cost after iteration 324: 0.149434\n",
      "Cost after iteration 325: 0.149202\n",
      "Cost after iteration 326: 0.148971\n",
      "Cost after iteration 327: 0.148741\n",
      "Cost after iteration 328: 0.148513\n",
      "Cost after iteration 329: 0.148285\n",
      "Cost after iteration 330: 0.148058\n",
      "Cost after iteration 331: 0.147832\n",
      "Cost after iteration 332: 0.147607\n",
      "Cost after iteration 333: 0.147383\n",
      "Cost after iteration 334: 0.147159\n",
      "Cost after iteration 335: 0.146937\n",
      "Cost after iteration 336: 0.146716\n",
      "Cost after iteration 337: 0.146496\n",
      "Cost after iteration 338: 0.146277\n",
      "Cost after iteration 339: 0.146058\n",
      "Cost after iteration 340: 0.145841\n",
      "Cost after iteration 341: 0.145624\n",
      "Cost after iteration 342: 0.145409\n",
      "Cost after iteration 343: 0.145194\n",
      "Cost after iteration 344: 0.144980\n",
      "Cost after iteration 345: 0.144767\n",
      "Cost after iteration 346: 0.144555\n",
      "Cost after iteration 347: 0.144344\n",
      "Cost after iteration 348: 0.144134\n",
      "Cost after iteration 349: 0.143924\n",
      "Cost after iteration 350: 0.143716\n",
      "Cost after iteration 351: 0.143508\n",
      "Cost after iteration 352: 0.143301\n",
      "Cost after iteration 353: 0.143095\n",
      "Cost after iteration 354: 0.142890\n",
      "Cost after iteration 355: 0.142685\n",
      "Cost after iteration 356: 0.142482\n",
      "Cost after iteration 357: 0.142279\n",
      "Cost after iteration 358: 0.142077\n",
      "Cost after iteration 359: 0.141876\n",
      "Cost after iteration 360: 0.141676\n",
      "Cost after iteration 361: 0.141476\n",
      "Cost after iteration 362: 0.141277\n",
      "Cost after iteration 363: 0.141079\n",
      "Cost after iteration 364: 0.140882\n",
      "Cost after iteration 365: 0.140686\n",
      "Cost after iteration 366: 0.140490\n",
      "Cost after iteration 367: 0.140295\n",
      "Cost after iteration 368: 0.140101\n",
      "Cost after iteration 369: 0.139908\n",
      "Cost after iteration 370: 0.139715\n",
      "Cost after iteration 371: 0.139524\n",
      "Cost after iteration 372: 0.139332\n",
      "Cost after iteration 373: 0.139142\n",
      "Cost after iteration 374: 0.138952\n",
      "Cost after iteration 375: 0.138764\n",
      "Cost after iteration 376: 0.138575\n",
      "Cost after iteration 377: 0.138388\n",
      "Cost after iteration 378: 0.138201\n",
      "Cost after iteration 379: 0.138015\n",
      "Cost after iteration 380: 0.137830\n",
      "Cost after iteration 381: 0.137645\n",
      "Cost after iteration 382: 0.137461\n",
      "Cost after iteration 383: 0.137278\n",
      "Cost after iteration 384: 0.137095\n",
      "Cost after iteration 385: 0.136913\n",
      "Cost after iteration 386: 0.136732\n",
      "Cost after iteration 387: 0.136551\n",
      "Cost after iteration 388: 0.136371\n",
      "Cost after iteration 389: 0.136192\n",
      "Cost after iteration 390: 0.136014\n",
      "Cost after iteration 391: 0.135836\n",
      "Cost after iteration 392: 0.135658\n",
      "Cost after iteration 393: 0.135482\n",
      "Cost after iteration 394: 0.135306\n",
      "Cost after iteration 395: 0.135130\n",
      "Cost after iteration 396: 0.134956\n",
      "Cost after iteration 397: 0.134782\n",
      "Cost after iteration 398: 0.134608\n",
      "Cost after iteration 399: 0.134435\n",
      "Cost after iteration 400: 0.134263\n",
      "Cost after iteration 401: 0.134091\n",
      "Cost after iteration 402: 0.133920\n",
      "Cost after iteration 403: 0.133750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 404: 0.133580\n",
      "Cost after iteration 405: 0.133411\n",
      "Cost after iteration 406: 0.133242\n",
      "Cost after iteration 407: 0.133074\n",
      "Cost after iteration 408: 0.132907\n",
      "Cost after iteration 409: 0.132740\n",
      "Cost after iteration 410: 0.132574\n",
      "Cost after iteration 411: 0.132408\n",
      "Cost after iteration 412: 0.132243\n",
      "Cost after iteration 413: 0.132079\n",
      "Cost after iteration 414: 0.131915\n",
      "Cost after iteration 415: 0.131751\n",
      "Cost after iteration 416: 0.131588\n",
      "Cost after iteration 417: 0.131426\n",
      "Cost after iteration 418: 0.131264\n",
      "Cost after iteration 419: 0.131103\n",
      "Cost after iteration 420: 0.130943\n",
      "Cost after iteration 421: 0.130783\n",
      "Cost after iteration 422: 0.130623\n",
      "Cost after iteration 423: 0.130464\n",
      "Cost after iteration 424: 0.130306\n",
      "Cost after iteration 425: 0.130148\n",
      "Cost after iteration 426: 0.129990\n",
      "Cost after iteration 427: 0.129834\n",
      "Cost after iteration 428: 0.129677\n",
      "Cost after iteration 429: 0.129521\n",
      "Cost after iteration 430: 0.129366\n",
      "Cost after iteration 431: 0.129211\n",
      "Cost after iteration 432: 0.129057\n",
      "Cost after iteration 433: 0.128903\n",
      "Cost after iteration 434: 0.128750\n",
      "Cost after iteration 435: 0.128597\n",
      "Cost after iteration 436: 0.128445\n",
      "Cost after iteration 437: 0.128293\n",
      "Cost after iteration 438: 0.128142\n",
      "Cost after iteration 439: 0.127991\n",
      "Cost after iteration 440: 0.127841\n",
      "Cost after iteration 441: 0.127691\n",
      "Cost after iteration 442: 0.127542\n",
      "Cost after iteration 443: 0.127393\n",
      "Cost after iteration 444: 0.127245\n",
      "Cost after iteration 445: 0.127097\n",
      "Cost after iteration 446: 0.126950\n",
      "Cost after iteration 447: 0.126803\n",
      "Cost after iteration 448: 0.126656\n",
      "Cost after iteration 449: 0.126510\n",
      "Cost after iteration 450: 0.126365\n",
      "Cost after iteration 451: 0.126220\n",
      "Cost after iteration 452: 0.126075\n",
      "Cost after iteration 453: 0.125931\n",
      "Cost after iteration 454: 0.125787\n",
      "Cost after iteration 455: 0.125644\n",
      "Cost after iteration 456: 0.125501\n",
      "Cost after iteration 457: 0.125359\n",
      "Cost after iteration 458: 0.125217\n",
      "Cost after iteration 459: 0.125075\n",
      "Cost after iteration 460: 0.124934\n",
      "Cost after iteration 461: 0.124794\n",
      "Cost after iteration 462: 0.124654\n",
      "Cost after iteration 463: 0.124514\n",
      "Cost after iteration 464: 0.124375\n",
      "Cost after iteration 465: 0.124236\n",
      "Cost after iteration 466: 0.124097\n",
      "Cost after iteration 467: 0.123959\n",
      "Cost after iteration 468: 0.123822\n",
      "Cost after iteration 469: 0.123685\n",
      "Cost after iteration 470: 0.123548\n",
      "Cost after iteration 471: 0.123412\n",
      "Cost after iteration 472: 0.123276\n",
      "Cost after iteration 473: 0.123140\n",
      "Cost after iteration 474: 0.123005\n",
      "Cost after iteration 475: 0.122870\n",
      "Cost after iteration 476: 0.122736\n",
      "Cost after iteration 477: 0.122602\n",
      "Cost after iteration 478: 0.122469\n",
      "Cost after iteration 479: 0.122335\n",
      "Cost after iteration 480: 0.122203\n",
      "Cost after iteration 481: 0.122070\n",
      "Cost after iteration 482: 0.121939\n",
      "Cost after iteration 483: 0.121807\n",
      "Cost after iteration 484: 0.121676\n",
      "Cost after iteration 485: 0.121545\n",
      "Cost after iteration 486: 0.121415\n",
      "Cost after iteration 487: 0.121285\n",
      "Cost after iteration 488: 0.121155\n",
      "Cost after iteration 489: 0.121026\n",
      "Cost after iteration 490: 0.120897\n",
      "Cost after iteration 491: 0.120769\n",
      "Cost after iteration 492: 0.120640\n",
      "Cost after iteration 493: 0.120513\n",
      "Cost after iteration 494: 0.120385\n",
      "Cost after iteration 495: 0.120258\n",
      "Cost after iteration 496: 0.120132\n",
      "Cost after iteration 497: 0.120005\n",
      "Cost after iteration 498: 0.119879\n",
      "Cost after iteration 499: 0.119754\n",
      "Cost after iteration 500: 0.119629\n",
      "Cost after iteration 501: 0.119504\n",
      "Cost after iteration 502: 0.119379\n",
      "Cost after iteration 503: 0.119255\n",
      "Cost after iteration 504: 0.119131\n",
      "Cost after iteration 505: 0.119008\n",
      "Cost after iteration 506: 0.118885\n",
      "Cost after iteration 507: 0.118762\n",
      "Cost after iteration 508: 0.118640\n",
      "Cost after iteration 509: 0.118518\n",
      "Cost after iteration 510: 0.118396\n",
      "Cost after iteration 511: 0.118275\n",
      "Cost after iteration 512: 0.118154\n",
      "Cost after iteration 513: 0.118033\n",
      "Cost after iteration 514: 0.117913\n",
      "Cost after iteration 515: 0.117793\n",
      "Cost after iteration 516: 0.117673\n",
      "Cost after iteration 517: 0.117553\n",
      "Cost after iteration 518: 0.117434\n",
      "Cost after iteration 519: 0.117316\n",
      "Cost after iteration 520: 0.117197\n",
      "Cost after iteration 521: 0.117079\n",
      "Cost after iteration 522: 0.116961\n",
      "Cost after iteration 523: 0.116844\n",
      "Cost after iteration 524: 0.116727\n",
      "Cost after iteration 525: 0.116610\n",
      "Cost after iteration 526: 0.116494\n",
      "Cost after iteration 527: 0.116378\n",
      "Cost after iteration 528: 0.116262\n",
      "Cost after iteration 529: 0.116146\n",
      "Cost after iteration 530: 0.116031\n",
      "Cost after iteration 531: 0.115916\n",
      "Cost after iteration 532: 0.115801\n",
      "Cost after iteration 533: 0.115687\n",
      "Cost after iteration 534: 0.115573\n",
      "Cost after iteration 535: 0.115459\n",
      "Cost after iteration 536: 0.115346\n",
      "Cost after iteration 537: 0.115233\n",
      "Cost after iteration 538: 0.115120\n",
      "Cost after iteration 539: 0.115008\n",
      "Cost after iteration 540: 0.114895\n",
      "Cost after iteration 541: 0.114784\n",
      "Cost after iteration 542: 0.114672\n",
      "Cost after iteration 543: 0.114561\n",
      "Cost after iteration 544: 0.114450\n",
      "Cost after iteration 545: 0.114339\n",
      "Cost after iteration 546: 0.114228\n",
      "Cost after iteration 547: 0.114118\n",
      "Cost after iteration 548: 0.114008\n",
      "Cost after iteration 549: 0.113899\n",
      "Cost after iteration 550: 0.113789\n",
      "Cost after iteration 551: 0.113680\n",
      "Cost after iteration 552: 0.113572\n",
      "Cost after iteration 553: 0.113463\n",
      "Cost after iteration 554: 0.113355\n",
      "Cost after iteration 555: 0.113247\n",
      "Cost after iteration 556: 0.113140\n",
      "Cost after iteration 557: 0.113032\n",
      "Cost after iteration 558: 0.112925\n",
      "Cost after iteration 559: 0.112818\n",
      "Cost after iteration 560: 0.112712\n",
      "Cost after iteration 561: 0.112605\n",
      "Cost after iteration 562: 0.112499\n",
      "Cost after iteration 563: 0.112394\n",
      "Cost after iteration 564: 0.112288\n",
      "Cost after iteration 565: 0.112183\n",
      "Cost after iteration 566: 0.112078\n",
      "Cost after iteration 567: 0.111973\n",
      "Cost after iteration 568: 0.111869\n",
      "Cost after iteration 569: 0.111765\n",
      "Cost after iteration 570: 0.111661\n",
      "Cost after iteration 571: 0.111557\n",
      "Cost after iteration 572: 0.111454\n",
      "Cost after iteration 573: 0.111351\n",
      "Cost after iteration 574: 0.111248\n",
      "Cost after iteration 575: 0.111145\n",
      "Cost after iteration 576: 0.111043\n",
      "Cost after iteration 577: 0.110941\n",
      "Cost after iteration 578: 0.110839\n",
      "Cost after iteration 579: 0.110738\n",
      "Cost after iteration 580: 0.110636\n",
      "Cost after iteration 581: 0.110535\n",
      "Cost after iteration 582: 0.110434\n",
      "Cost after iteration 583: 0.110334\n",
      "Cost after iteration 584: 0.110233\n",
      "Cost after iteration 585: 0.110133\n",
      "Cost after iteration 586: 0.110033\n",
      "Cost after iteration 587: 0.109934\n",
      "Cost after iteration 588: 0.109834\n",
      "Cost after iteration 589: 0.109735\n",
      "Cost after iteration 590: 0.109636\n",
      "Cost after iteration 591: 0.109537\n",
      "Cost after iteration 592: 0.109439\n",
      "Cost after iteration 593: 0.109341\n",
      "Cost after iteration 594: 0.109243\n",
      "Cost after iteration 595: 0.109145\n",
      "Cost after iteration 596: 0.109048\n",
      "Cost after iteration 597: 0.108950\n",
      "Cost after iteration 598: 0.108853\n",
      "Cost after iteration 599: 0.108756\n",
      "Cost after iteration 600: 0.108660\n",
      "Cost after iteration 601: 0.108563\n",
      "Cost after iteration 602: 0.108467\n",
      "Cost after iteration 603: 0.108371\n",
      "Cost after iteration 604: 0.108276\n",
      "Cost after iteration 605: 0.108180\n",
      "Cost after iteration 606: 0.108085\n",
      "Cost after iteration 607: 0.107990\n",
      "Cost after iteration 608: 0.107895\n",
      "Cost after iteration 609: 0.107801\n",
      "Cost after iteration 610: 0.107706\n",
      "Cost after iteration 611: 0.107612\n",
      "Cost after iteration 612: 0.107518\n",
      "Cost after iteration 613: 0.107425\n",
      "Cost after iteration 614: 0.107331\n",
      "Cost after iteration 615: 0.107238\n",
      "Cost after iteration 616: 0.107145\n",
      "Cost after iteration 617: 0.107052\n",
      "Cost after iteration 618: 0.106959\n",
      "Cost after iteration 619: 0.106867\n",
      "Cost after iteration 620: 0.106775\n",
      "Cost after iteration 621: 0.106683\n",
      "Cost after iteration 622: 0.106591\n",
      "Cost after iteration 623: 0.106500\n",
      "Cost after iteration 624: 0.106408\n",
      "Cost after iteration 625: 0.106317\n",
      "Cost after iteration 626: 0.106226\n",
      "Cost after iteration 627: 0.106135\n",
      "Cost after iteration 628: 0.106045\n",
      "Cost after iteration 629: 0.105955\n",
      "Cost after iteration 630: 0.105864\n",
      "Cost after iteration 631: 0.105775\n",
      "Cost after iteration 632: 0.105685\n",
      "Cost after iteration 633: 0.105595\n",
      "Cost after iteration 634: 0.105506\n",
      "Cost after iteration 635: 0.105417\n",
      "Cost after iteration 636: 0.105328\n",
      "Cost after iteration 637: 0.105239\n",
      "Cost after iteration 638: 0.105151\n",
      "Cost after iteration 639: 0.105062\n",
      "Cost after iteration 640: 0.104974\n",
      "Cost after iteration 641: 0.104886\n",
      "Cost after iteration 642: 0.104799\n",
      "Cost after iteration 643: 0.104711\n",
      "Cost after iteration 644: 0.104624\n",
      "Cost after iteration 645: 0.104537\n",
      "Cost after iteration 646: 0.104450\n",
      "Cost after iteration 647: 0.104363\n",
      "Cost after iteration 648: 0.104276\n",
      "Cost after iteration 649: 0.104190\n",
      "Cost after iteration 650: 0.104104\n",
      "Cost after iteration 651: 0.104018\n",
      "Cost after iteration 652: 0.103932\n",
      "Cost after iteration 653: 0.103846\n",
      "Cost after iteration 654: 0.103761\n",
      "Cost after iteration 655: 0.103675\n",
      "Cost after iteration 656: 0.103590\n",
      "Cost after iteration 657: 0.103505\n",
      "Cost after iteration 658: 0.103421\n",
      "Cost after iteration 659: 0.103336\n",
      "Cost after iteration 660: 0.103252\n",
      "Cost after iteration 661: 0.103168\n",
      "Cost after iteration 662: 0.103084\n",
      "Cost after iteration 663: 0.103000\n",
      "Cost after iteration 664: 0.102916\n",
      "Cost after iteration 665: 0.102833\n",
      "Cost after iteration 666: 0.102749\n",
      "Cost after iteration 667: 0.102666\n",
      "Cost after iteration 668: 0.102583\n",
      "Cost after iteration 669: 0.102500\n",
      "Cost after iteration 670: 0.102418\n",
      "Cost after iteration 671: 0.102336\n",
      "Cost after iteration 672: 0.102253\n",
      "Cost after iteration 673: 0.102171\n",
      "Cost after iteration 674: 0.102089\n",
      "Cost after iteration 675: 0.102008\n",
      "Cost after iteration 676: 0.101926\n",
      "Cost after iteration 677: 0.101845\n",
      "Cost after iteration 678: 0.101763\n",
      "Cost after iteration 679: 0.101682\n",
      "Cost after iteration 680: 0.101602\n",
      "Cost after iteration 681: 0.101521\n",
      "Cost after iteration 682: 0.101440\n",
      "Cost after iteration 683: 0.101360\n",
      "Cost after iteration 684: 0.101280\n",
      "Cost after iteration 685: 0.101200\n",
      "Cost after iteration 686: 0.101120\n",
      "Cost after iteration 687: 0.101040\n",
      "Cost after iteration 688: 0.100961\n",
      "Cost after iteration 689: 0.100881\n",
      "Cost after iteration 690: 0.100802\n",
      "Cost after iteration 691: 0.100723\n",
      "Cost after iteration 692: 0.100644\n",
      "Cost after iteration 693: 0.100565\n",
      "Cost after iteration 694: 0.100487\n",
      "Cost after iteration 695: 0.100408\n",
      "Cost after iteration 696: 0.100330\n",
      "Cost after iteration 697: 0.100252\n",
      "Cost after iteration 698: 0.100174\n",
      "Cost after iteration 699: 0.100096\n",
      "Cost after iteration 700: 0.100019\n",
      "Cost after iteration 701: 0.099941\n",
      "Cost after iteration 702: 0.099864\n",
      "Cost after iteration 703: 0.099787\n",
      "Cost after iteration 704: 0.099710\n",
      "Cost after iteration 705: 0.099633\n",
      "Cost after iteration 706: 0.099556\n",
      "Cost after iteration 707: 0.099480\n",
      "Cost after iteration 708: 0.099404\n",
      "Cost after iteration 709: 0.099327\n",
      "Cost after iteration 710: 0.099251\n",
      "Cost after iteration 711: 0.099175\n",
      "Cost after iteration 712: 0.099100\n",
      "Cost after iteration 713: 0.099024\n",
      "Cost after iteration 714: 0.098948\n",
      "Cost after iteration 715: 0.098873\n",
      "Cost after iteration 716: 0.098798\n",
      "Cost after iteration 717: 0.098723\n",
      "Cost after iteration 718: 0.098648\n",
      "Cost after iteration 719: 0.098573\n",
      "Cost after iteration 720: 0.098499\n",
      "Cost after iteration 721: 0.098424\n",
      "Cost after iteration 722: 0.098350\n",
      "Cost after iteration 723: 0.098276\n",
      "Cost after iteration 724: 0.098202\n",
      "Cost after iteration 725: 0.098128\n",
      "Cost after iteration 726: 0.098054\n",
      "Cost after iteration 727: 0.097981\n",
      "Cost after iteration 728: 0.097907\n",
      "Cost after iteration 729: 0.097834\n",
      "Cost after iteration 730: 0.097761\n",
      "Cost after iteration 731: 0.097688\n",
      "Cost after iteration 732: 0.097615\n",
      "Cost after iteration 733: 0.097543\n",
      "Cost after iteration 734: 0.097470\n",
      "Cost after iteration 735: 0.097398\n",
      "Cost after iteration 736: 0.097325\n",
      "Cost after iteration 737: 0.097253\n",
      "Cost after iteration 738: 0.097181\n",
      "Cost after iteration 739: 0.097109\n",
      "Cost after iteration 740: 0.097037\n",
      "Cost after iteration 741: 0.096966\n",
      "Cost after iteration 742: 0.096894\n",
      "Cost after iteration 743: 0.096823\n",
      "Cost after iteration 744: 0.096752\n",
      "Cost after iteration 745: 0.096681\n",
      "Cost after iteration 746: 0.096610\n",
      "Cost after iteration 747: 0.096539\n",
      "Cost after iteration 748: 0.096468\n",
      "Cost after iteration 749: 0.096398\n",
      "Cost after iteration 750: 0.096328\n",
      "Cost after iteration 751: 0.096257\n",
      "Cost after iteration 752: 0.096187\n",
      "Cost after iteration 753: 0.096117\n",
      "Cost after iteration 754: 0.096047\n",
      "Cost after iteration 755: 0.095978\n",
      "Cost after iteration 756: 0.095908\n",
      "Cost after iteration 757: 0.095839\n",
      "Cost after iteration 758: 0.095769\n",
      "Cost after iteration 759: 0.095700\n",
      "Cost after iteration 760: 0.095631\n",
      "Cost after iteration 761: 0.095562\n",
      "Cost after iteration 762: 0.095493\n",
      "Cost after iteration 763: 0.095425\n",
      "Cost after iteration 764: 0.095356\n",
      "Cost after iteration 765: 0.095288\n",
      "Cost after iteration 766: 0.095219\n",
      "Cost after iteration 767: 0.095151\n",
      "Cost after iteration 768: 0.095083\n",
      "Cost after iteration 769: 0.095015\n",
      "Cost after iteration 770: 0.094947\n",
      "Cost after iteration 771: 0.094880\n",
      "Cost after iteration 772: 0.094812\n",
      "Cost after iteration 773: 0.094745\n",
      "Cost after iteration 774: 0.094677\n",
      "Cost after iteration 775: 0.094610\n",
      "Cost after iteration 776: 0.094543\n",
      "Cost after iteration 777: 0.094476\n",
      "Cost after iteration 778: 0.094409\n",
      "Cost after iteration 779: 0.094342\n",
      "Cost after iteration 780: 0.094276\n",
      "Cost after iteration 781: 0.094209\n",
      "Cost after iteration 782: 0.094143\n",
      "Cost after iteration 783: 0.094077\n",
      "Cost after iteration 784: 0.094011\n",
      "Cost after iteration 785: 0.093945\n",
      "Cost after iteration 786: 0.093879\n",
      "Cost after iteration 787: 0.093813\n",
      "Cost after iteration 788: 0.093748\n",
      "Cost after iteration 789: 0.093682\n",
      "Cost after iteration 790: 0.093617\n",
      "Cost after iteration 791: 0.093551\n",
      "Cost after iteration 792: 0.093486\n",
      "Cost after iteration 793: 0.093421\n",
      "Cost after iteration 794: 0.093356\n",
      "Cost after iteration 795: 0.093291\n",
      "Cost after iteration 796: 0.093227\n",
      "Cost after iteration 797: 0.093162\n",
      "Cost after iteration 798: 0.093098\n",
      "Cost after iteration 799: 0.093033\n",
      "Cost after iteration 800: 0.092969\n",
      "Cost after iteration 801: 0.092905\n",
      "Cost after iteration 802: 0.092841\n",
      "Cost after iteration 803: 0.092777\n",
      "Cost after iteration 804: 0.092713\n",
      "Cost after iteration 805: 0.092650\n",
      "Cost after iteration 806: 0.092586\n",
      "Cost after iteration 807: 0.092523\n",
      "Cost after iteration 808: 0.092459\n",
      "Cost after iteration 809: 0.092396\n",
      "Cost after iteration 810: 0.092333\n",
      "Cost after iteration 811: 0.092270\n",
      "Cost after iteration 812: 0.092207\n",
      "Cost after iteration 813: 0.092144\n",
      "Cost after iteration 814: 0.092082\n",
      "Cost after iteration 815: 0.092019\n",
      "Cost after iteration 816: 0.091957\n",
      "Cost after iteration 817: 0.091894\n",
      "Cost after iteration 818: 0.091832\n",
      "Cost after iteration 819: 0.091770\n",
      "Cost after iteration 820: 0.091708\n",
      "Cost after iteration 821: 0.091646\n",
      "Cost after iteration 822: 0.091584\n",
      "Cost after iteration 823: 0.091522\n",
      "Cost after iteration 824: 0.091461\n",
      "Cost after iteration 825: 0.091399\n",
      "Cost after iteration 826: 0.091338\n",
      "Cost after iteration 827: 0.091277\n",
      "Cost after iteration 828: 0.091215\n",
      "Cost after iteration 829: 0.091154\n",
      "Cost after iteration 830: 0.091093\n",
      "Cost after iteration 831: 0.091032\n",
      "Cost after iteration 832: 0.090972\n",
      "Cost after iteration 833: 0.090911\n",
      "Cost after iteration 834: 0.090850\n",
      "Cost after iteration 835: 0.090790\n",
      "Cost after iteration 836: 0.090730\n",
      "Cost after iteration 837: 0.090669\n",
      "Cost after iteration 838: 0.090609\n",
      "Cost after iteration 839: 0.090549\n",
      "Cost after iteration 840: 0.090489\n",
      "Cost after iteration 841: 0.090429\n",
      "Cost after iteration 842: 0.090370\n",
      "Cost after iteration 843: 0.090310\n",
      "Cost after iteration 844: 0.090250\n",
      "Cost after iteration 845: 0.090191\n",
      "Cost after iteration 846: 0.090132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 847: 0.090072\n",
      "Cost after iteration 848: 0.090013\n",
      "Cost after iteration 849: 0.089954\n",
      "Cost after iteration 850: 0.089895\n",
      "Cost after iteration 851: 0.089836\n",
      "Cost after iteration 852: 0.089778\n",
      "Cost after iteration 853: 0.089719\n",
      "Cost after iteration 854: 0.089660\n",
      "Cost after iteration 855: 0.089602\n",
      "Cost after iteration 856: 0.089544\n",
      "Cost after iteration 857: 0.089485\n",
      "Cost after iteration 858: 0.089427\n",
      "Cost after iteration 859: 0.089369\n",
      "Cost after iteration 860: 0.089311\n",
      "Cost after iteration 861: 0.089253\n",
      "Cost after iteration 862: 0.089195\n",
      "Cost after iteration 863: 0.089138\n",
      "Cost after iteration 864: 0.089080\n",
      "Cost after iteration 865: 0.089023\n",
      "Cost after iteration 866: 0.088965\n",
      "Cost after iteration 867: 0.088908\n",
      "Cost after iteration 868: 0.088851\n",
      "Cost after iteration 869: 0.088793\n",
      "Cost after iteration 870: 0.088736\n",
      "Cost after iteration 871: 0.088679\n",
      "Cost after iteration 872: 0.088623\n",
      "Cost after iteration 873: 0.088566\n",
      "Cost after iteration 874: 0.088509\n",
      "Cost after iteration 875: 0.088453\n",
      "Cost after iteration 876: 0.088396\n",
      "Cost after iteration 877: 0.088340\n",
      "Cost after iteration 878: 0.088283\n",
      "Cost after iteration 879: 0.088227\n",
      "Cost after iteration 880: 0.088171\n",
      "Cost after iteration 881: 0.088115\n",
      "Cost after iteration 882: 0.088059\n",
      "Cost after iteration 883: 0.088003\n",
      "Cost after iteration 884: 0.087947\n",
      "Cost after iteration 885: 0.087892\n",
      "Cost after iteration 886: 0.087836\n",
      "Cost after iteration 887: 0.087781\n",
      "Cost after iteration 888: 0.087725\n",
      "Cost after iteration 889: 0.087670\n",
      "Cost after iteration 890: 0.087615\n",
      "Cost after iteration 891: 0.087560\n",
      "Cost after iteration 892: 0.087504\n",
      "Cost after iteration 893: 0.087449\n",
      "Cost after iteration 894: 0.087395\n",
      "Cost after iteration 895: 0.087340\n",
      "Cost after iteration 896: 0.087285\n",
      "Cost after iteration 897: 0.087230\n",
      "Cost after iteration 898: 0.087176\n",
      "Cost after iteration 899: 0.087121\n",
      "Cost after iteration 900: 0.087067\n",
      "Cost after iteration 901: 0.087013\n",
      "Cost after iteration 902: 0.086959\n",
      "Cost after iteration 903: 0.086905\n",
      "Cost after iteration 904: 0.086850\n",
      "Cost after iteration 905: 0.086797\n",
      "Cost after iteration 906: 0.086743\n",
      "Cost after iteration 907: 0.086689\n",
      "Cost after iteration 908: 0.086635\n",
      "Cost after iteration 909: 0.086582\n",
      "Cost after iteration 910: 0.086528\n",
      "Cost after iteration 911: 0.086475\n",
      "Cost after iteration 912: 0.086421\n",
      "Cost after iteration 913: 0.086368\n",
      "Cost after iteration 914: 0.086315\n",
      "Cost after iteration 915: 0.086262\n",
      "Cost after iteration 916: 0.086209\n",
      "Cost after iteration 917: 0.086156\n",
      "Cost after iteration 918: 0.086103\n",
      "Cost after iteration 919: 0.086050\n",
      "Cost after iteration 920: 0.085997\n",
      "Cost after iteration 921: 0.085945\n",
      "Cost after iteration 922: 0.085892\n",
      "Cost after iteration 923: 0.085840\n",
      "Cost after iteration 924: 0.085787\n",
      "Cost after iteration 925: 0.085735\n",
      "Cost after iteration 926: 0.085683\n",
      "Cost after iteration 927: 0.085631\n",
      "Cost after iteration 928: 0.085579\n",
      "Cost after iteration 929: 0.085527\n",
      "Cost after iteration 930: 0.085475\n",
      "Cost after iteration 931: 0.085423\n",
      "Cost after iteration 932: 0.085371\n",
      "Cost after iteration 933: 0.085320\n",
      "Cost after iteration 934: 0.085268\n",
      "Cost after iteration 935: 0.085216\n",
      "Cost after iteration 936: 0.085165\n",
      "Cost after iteration 937: 0.085114\n",
      "Cost after iteration 938: 0.085062\n",
      "Cost after iteration 939: 0.085011\n",
      "Cost after iteration 940: 0.084960\n",
      "Cost after iteration 941: 0.084909\n",
      "Cost after iteration 942: 0.084858\n",
      "Cost after iteration 943: 0.084807\n",
      "Cost after iteration 944: 0.084756\n",
      "Cost after iteration 945: 0.084706\n",
      "Cost after iteration 946: 0.084655\n",
      "Cost after iteration 947: 0.084604\n",
      "Cost after iteration 948: 0.084554\n",
      "Cost after iteration 949: 0.084503\n",
      "Cost after iteration 950: 0.084453\n",
      "Cost after iteration 951: 0.084403\n",
      "Cost after iteration 952: 0.084353\n",
      "Cost after iteration 953: 0.084302\n",
      "Cost after iteration 954: 0.084252\n",
      "Cost after iteration 955: 0.084202\n",
      "Cost after iteration 956: 0.084152\n",
      "Cost after iteration 957: 0.084103\n",
      "Cost after iteration 958: 0.084053\n",
      "Cost after iteration 959: 0.084003\n",
      "Cost after iteration 960: 0.083954\n",
      "Cost after iteration 961: 0.083904\n",
      "Cost after iteration 962: 0.083855\n",
      "Cost after iteration 963: 0.083805\n",
      "Cost after iteration 964: 0.083756\n",
      "Cost after iteration 965: 0.083707\n",
      "Cost after iteration 966: 0.083657\n",
      "Cost after iteration 967: 0.083608\n",
      "Cost after iteration 968: 0.083559\n",
      "Cost after iteration 969: 0.083510\n",
      "Cost after iteration 970: 0.083461\n",
      "Cost after iteration 971: 0.083413\n",
      "Cost after iteration 972: 0.083364\n",
      "Cost after iteration 973: 0.083315\n",
      "Cost after iteration 974: 0.083267\n",
      "Cost after iteration 975: 0.083218\n",
      "Cost after iteration 976: 0.083170\n",
      "Cost after iteration 977: 0.083121\n",
      "Cost after iteration 978: 0.083073\n",
      "Cost after iteration 979: 0.083025\n",
      "Cost after iteration 980: 0.082976\n",
      "Cost after iteration 981: 0.082928\n",
      "Cost after iteration 982: 0.082880\n",
      "Cost after iteration 983: 0.082832\n",
      "Cost after iteration 984: 0.082784\n",
      "Cost after iteration 985: 0.082736\n",
      "Cost after iteration 986: 0.082689\n",
      "Cost after iteration 987: 0.082641\n",
      "Cost after iteration 988: 0.082593\n",
      "Cost after iteration 989: 0.082546\n",
      "Cost after iteration 990: 0.082498\n",
      "Cost after iteration 991: 0.082451\n",
      "Cost after iteration 992: 0.082403\n",
      "Cost after iteration 993: 0.082356\n",
      "Cost after iteration 994: 0.082309\n",
      "Cost after iteration 995: 0.082262\n",
      "Cost after iteration 996: 0.082215\n",
      "Cost after iteration 997: 0.082168\n",
      "Cost after iteration 998: 0.082121\n",
      "Cost after iteration 999: 0.082074\n",
      "[[-0.08396846]]\n",
      "train accuracy: 97.66355140186916 %\n",
      "test accuracy: 97.5103734439834 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1000, learning_rate = 0.01, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's now plot the cost function curve with number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8HWd97/HPV5sl2ZbkIzvxbong\nNBBnI7YMJOHSFloDbdJSSpOylLY0hUuaAO3lhraXUnq5txdoyxYKgQboAoGG0hoIhEKhSaCJLWdx\n4gQnxkus2E7kTV5lSdbv/jEj+Vg5WryMRtL5vl8vvaQz85w5vzmJz/c888w8o4jAzMwMoCLvAszM\nbOJwKJiZ2SCHgpmZDXIomJnZIIeCmZkNciiYmdkgh4KVBUnflvRbeddhNtE5FCxTkrZKekXedUTE\nqyLii3nXASDph5LeOg6vM03SbZIOSNol6d2jtH9X2q4rfd60onV/IekRSX2S3p917ZYfh4JNepKq\n8q5hwESqBXg/sBRYAvws8B5Jq0o1lPSLwM3AzwMtwPOAPy9qsgl4D/Ct7Mq1icChYLmR9EuSHpK0\nX9KPJV1ctO5mST+VdFDSY5J+tWjdWyT9SNLfSNoLvD9ddq+kj0jaJ2mLpFcVPWfw2/kY2rZKujt9\n7e9JukXSPw6zDy+X1CHpf0raBXxe0ixJ35TUmW7/m5IWpu0/CFwFfFLSIUmfTJdfIOnfJe2VtFHS\n68/CW/xm4C8iYl9EPA58FnjLMG1/C/i7iNgQEfuAvyhuGxFfjIhvAwfPQl02gTkULBeSXgTcBvw+\n0Ax8BlhddMjipyQfno0k31j/UdK8ok2sBDYD5wAfLFq2EZgNfAj4O0kapoSR2n4JWJPW9X7gTaPs\nzlygQPKN/HqSf1efTx8vBo4CnwSIiD8B7gFuiIgZEXGDpOnAv6evew5wHfApSReWejFJn0qDtNTP\n+rTNLGA+8HDRUx8GSm4zXT607bmSmkfZd5tiHAqWl98DPhMR90fE8fR4/zHgxQAR8c8RsSMi+iPi\nK8CTQFvR83dExCcioi8ijqbLtkXEZyPiOPBFYB5w7jCvX7KtpMXACuB9EdETEfcCq0fZl37gzyLi\nWEQcjYg9EfG1iDgSEQdJQuu/jfD8XwK2RsTn0/15APga8LpSjSPiv0dE0zA/A72tGenvrqKndgEz\nh6lhRom2jNDepiiHguVlCfCHxd9ygUUk326R9OaiQ0v7gWUk3+oHbC+xzV0Df0TEkfTPGSXajdR2\nPrC3aNlwr1WsMyK6Bx5Iqpf0GUnbJB0A7gaaJFUO8/wlwMoh78UbSHogp+tQ+ruhaFkDwx/+OVSi\nLSO0tynKoWB52Q58cMi33PqI+LKkJSTHv28AmiOiCXgUKD4UlNX0vjuBgqT6omWLRnnO0Fr+EPgZ\nYGVENAAvS5drmPbbgf8c8l7MiIi3l3oxSZ9OxyNK/WwASMcFdgKXFD31EmDDMPuwoUTbZyJiz/C7\nbVORQ8HGQ7Wk2qKfKpIP/bdJWqnEdEmvkTQTmE7ywdkJIOm3SXoKmYuIbUA7yeB1jaSXAL98ipuZ\nSTKOsF9SAfizIeufITm7Z8A3gfMlvUlSdfqzQtILhqnxbWlolPopHjP4e+BP04HvC0gO2X1hmJr/\nHvhdSS9MxyP+tLhtWlMtyWdGVfrfcbiej01iDgUbD3eSfEgO/Lw/ItpJPqQ+CewjOeXxLQAR8Rjw\nV8B/kXyAXgT8aBzrfQPwEmAP8L+Br5CMd4zVR4E6YDdwH/CdIes/BrwuPTPp4+m4wy8A1wI7SA5t\n/T9gGmfmz0gG7LcB/wl8OCK+AyBpcdqzWAyQLv8Q8IO0/TZODrPPkvy3uw74k/Tv0QbgbRKSb7Jj\nNjJJXwF+EhFDv/GbTTnuKZgNkR66OU9ShZKLva4B/jXvuszGw0S6+tJsopgL/AvJdQodwNsj4sF8\nSzIbHz58ZGZmg3z4yMzMBk26w0ezZ8+OlpaWvMswM5tU1q1btzsi5ozWbtKFQktLC+3t7XmXYWY2\nqUjaNpZ2PnxkZmaDHApmZjbIoWBmZoMyDQVJq9IbhmySdHOJ9X+TzoT5kKQn0tkhzcwsJ5kNNKeT\nZd0CvJLkAqC1klan89oAEBHvKmr/B8BlWdVjZmajy7Kn0AZsiojNEdED3E4yXcBwrgO+nGE9ZmY2\niixDYQEn35ykI132HOn8+a3Afwyz/npJ7ZLaOzs7z3qhZmaWyDIUSt0bd7g5Na4F7khvjfjcJ0Xc\nGhHLI2L5nDmjXntR0rpte/nQd35yWs81MysXWYZCByffsWohyVzxpVxLxoeONuw4wKd++FM69h0Z\nvbGZWZnKMhTWAksltUqqIfngf84N0CX9DDCL5IYqmWlrLQCwZsveLF/GzGxSyywUIqKP5B67dwGP\nA1+NiA2SPiDp6qKm1wG3R8bTtZ5/zkwa66odCmZmI8h07qOIuJPkVozFy9435PH7s6xhQEWFWNFS\ncCiYmY2grK5oXtlaYPPuwzx7sDvvUszMJqSyCoWBcYW1W/blXImZ2cRUVqFw4fwG6msqWbNlT96l\nmJlNSGUVClWVFVy+ZBb3e1zBzKyksgoFgLaWAhufOcj+Iz15l2JmNuGUXyi0FoiA9q0eVzAzG6rs\nQuGSRU3UVFawZqsPIZmZDVV2oVBbXcmli5o8rmBmVkLZhQIkh5AefbqLw8f68i7FzGxCKdtQON4f\nPPCUxxXMzIqVZSi8aMksKivkKS/MzIYoy1CYMa2KZfMbPK5gZjZEWYYCJIeQHtq+n+7ekvf1MTMr\nS2UcCs309PWzvqMr71LMzCaMsg2FFS2zADwPkplZkbINhab6Gi6YO9PjCmZmRco2FCAZV1i3bR99\nx/vzLsXMbEIo+1A40nOcDTsO5F2KmdmEUN6h0JLcdMfXK5iZJco6FM5pqKV19nSPK5iZpco6FCDp\nLazdupf+/si7FDOz3DkUWgt0He3liWcP5l2KmVnuHAqtHlcwMxuQaShIWiVpo6RNkm4eps3rJT0m\naYOkL2VZTykLZ9Uxv7HW4wpmZkBVVhuWVAncArwS6ADWSlodEY8VtVkKvBe4IiL2STonq3pGqJO2\n1gI/+ukeIgJJ412CmdmEkWVPoQ3YFBGbI6IHuB24Zkib3wNuiYh9ABHxbIb1DKuttZnOg8fYuudI\nHi9vZjZhZBkKC4DtRY870mXFzgfOl/QjSfdJWpVhPcM6Ma7geZDMrLxlGQqljsMMPe+zClgKvBy4\nDvicpKbnbEi6XlK7pPbOzs6zXuh5c6bTPL3G4wpmVvayDIUOYFHR44XAjhJt/i0ieiNiC7CRJCRO\nEhG3RsTyiFg+Z86cs17owLiCz0Ays3KXZSisBZZKapVUA1wLrB7S5l+BnwWQNJvkcNLmDGsaVltr\ngY59R3l6/9E8Xt7MbELILBQiog+4AbgLeBz4akRskPQBSVenze4C9kh6DPgB8D8iIpcD+wPjCmvd\nWzCzMpbZKakAEXEncOeQZe8r+juAd6c/ubpgbgMza6u4f8tefuWyoePhZmbloeyvaB5QWSFWtBR8\nBpKZlTWHQpG21gI/7TzM7kPH8i7FzCwXDoUiHlcws3LnUCiybH4jddWVvl7BzMqWQ6FITVUFL1rS\n5OsVzKxsORSGaGtp5vFdBzjQ3Zt3KWZm486hMERba4EIWLd1X96lmJmNO4fCEJctbqK6Uh5XMLOy\n5FAYora6kksWNvl6BTMrSw6FEtpaC6zv6OJoz/G8SzEzG1cOhRLaWgv09QcPPuVxBTMrLw6FEi5f\nMosK4XEFMys7DoUSZtZWc+H8Rl+vYGZlx6EwjLbWAg88tY+evv68SzEzGzcOhWG0tRY41tfPI0/v\nz7sUM7Nx41AYxoqWZHI8jyuYWTlxKAyjML2G88+d4XEFMysrDoURtLUWaN+6j+P9kXcpZmbjwqEw\nghUtBQ4d6+PxnQfyLsXMbFw4FEYwcNMdjyuYWblwKIxgXmMdiwv1ngfJzMqGQ2EUba0F1mzZS4TH\nFcxs6nMojKKttcC+I71sevZQ3qWYmWXOoTCKlR5XMLMykmkoSFolaaOkTZJuLrH+LZI6JT2U/rw1\ny3pOx+JCPec2TPP1CmZWFqqy2rCkSuAW4JVAB7BW0uqIeGxI069ExA1Z1XGmJNHW2jw4riAp75LM\nzDKTZU+hDdgUEZsjoge4Hbgmw9fLTFtrgV0Hutm+92jepZiZZSrLUFgAbC963JEuG+rXJK2XdIek\nRaU2JOl6Se2S2js7O7OodUQnxhV8aqqZTW1ZhkKp4yxDz+v8BtASERcD3wO+WGpDEXFrRCyPiOVz\n5sw5y2WO7vlzZjCrvtrjCmY25WUZCh1A8Tf/hcCO4gYRsScijqUPPwtcnmE9p62iQqxoKbBmq0PB\nzKa2LENhLbBUUqukGuBaYHVxA0nzih5eDTyeYT1npK21wLY9R9jV1Z13KWZmmcksFCKiD7gBuIvk\nw/6rEbFB0gckXZ02u1HSBkkPAzcCb8mqnjO1srUZwL0FM5vSMjslFSAi7gTuHLLsfUV/vxd4b5Y1\nnC0vmDeTGdOqWLNlD1dfMj/vcszMMuErmseoqrKCy5fM8mCzmU1pDoVT0NZa4IlnDrH3cE/epZiZ\nZcKhcAoGrldY63EFM5uiHAqn4KKFjUyrqvAhJDObshwKp2BaVSWXLW5yKJjZlOVQOEVtrc1s2NHF\nwe7evEsxMzvrHAqnaGVrgf6Addv25V2KmdlZ51A4RZctbqKqQj6EZGZTkkPhFNXXVHHRwkaHgplN\nSQ6F09DWWuDhjv109x7PuxQzs7PKoXAaVrYW6D0ePPjU/rxLMTM7qxwKp+HyJQUkfAjJzKYch8Jp\naKyr5gVzG1iz1XdiM7OpxaFwmtpaC6zbto+evv68SzEzO2scCqdpZWuB7t5+Ht3RlXcpZmZnjUPh\nNK0YmBzP4wpmNoU4FE7T7BnTOG/OdA82m9mU4lA4A22tzazZupfj/ZF3KWZmZ4VD4QysbC1wsLuP\njbsO5l2KmdlZ4VA4A23puMKaLT411cymBofCGZjfVMfCWXWs8Z3YzGyKcCicobbWAmu27CXC4wpm\nNvmNKRQk/fpYlpWjla0Fdh/qYfPuw3mXYmZ2xsbaU3jvGJeVnbbWZsDzIJnZ1DBiKEh6laRPAAsk\nfbzo5wtA32gbl7RK0kZJmyTdPEK710kKSctPeQ9y1tJcz5yZ0xwKZjYlVI2yfgfQDlwNrCtafhB4\n10hPlFQJ3AK8EugA1kpaHRGPDWk3E7gRuP/USp8YJA2OK5iZTXYj9hQi4uGI+CLw/Ij4Yvr3amBT\nRIx2k+K2tN3miOgBbgeuKdHuL4APAd2nXv7EsLK1wNP7j9Kx70jepZiZnZGxjin8u6QGSQXgYeDz\nkv56lOcsALYXPe5Ilw2SdBmwKCK+OdKGJF0vqV1Se2dn5xhLHj8nrldwb8HMJrexhkJjRBwAXgt8\nPiIuB14xynNUYtngeZuSKoC/Af5wtBePiFsjYnlELJ8zZ84YSx4/558zk8a6aoeCmU16Yw2FKknz\ngNcDI36rL9IBLCp6vJBkjGLATGAZ8ENJW4EXA6sn42BzRYVY0eJxBTOb/MYaCh8A7gJ+GhFrJT0P\neHKU56wFlkpqlVQDXEsyHgFARHRFxOyIaImIFuA+4OqIaD/lvZgAVrYW2Lz7MM8enLRDI2ZmYwuF\niPjniLg4It6ePt4cEb82ynP6gBtIwuRx4KsRsUHSByRdfaaFTzRtg/dXGG383cxs4hrtlFQAJC0E\nPgFcQTIucC9wU0R0jPS8iLgTuHPIsvcN0/blY6llorpwfgP1NZWs2bKH11w8L+9yzMxOy1gPH32e\n5NDPfJIziL6RLrNUVWUFly+Zxf0eVzCzSWysoTAnIj4fEX3pzxeAiXcaUM7aWgpsfOYg+4/05F2K\nmdlpGWso7Jb0RkmV6c8bAd9EYIi21gIR0L7V4wpmNjmNNRR+h+R01F3ATuB1wG9nVdRkdcmiJmoq\nK3x/BTObtMY00EwyFcVvDUxtkV7Z/BGSsLBUbXUlly5q8riCmU1aY+0pXFw811FE7AUuy6akya2t\ntcCjT3dx+Niok8iamU04Yw2FCkmzBh6kPYWx9jLKSltrgeP9wQNPeVzBzCafsX6w/xXwY0l3kFyn\n8Hrgg5lVNYm9aMksKivEmi17uWqpT9Ays8llTKEQEX8vqR34OZKJ7l479L4IlpgxrYpl8xs8rmBm\nk9KYDwGlIeAgGIO21gJf/K9tdPcep7a6Mu9yzMzGbKxjCnYK2lqb6enrZ31HV96lmJmdEodCBla0\nJGPya7b4+j4zm1wcChloqq/hgrkzPa5gZpOOQyEjba0F1m3bR9/x/rxLMTMbM4dCRtpaCxzpOc6G\nHQfyLsXMbMwcChlpa0luuuNbdJrZZOJQyMg5DbW0zp7ucQUzm1QcChlqaymwdute+vsj71LMzMbE\noZChttYCXUd7eeLZg3mXYmY2Jg6FDLW1elzBzCYXh0KGFs6qY35jrccVzGzScChkSBJtrQXWbNlL\nhMcVzGzicyhkrK21mc6Dx9i650jepZiZjSrTUJC0StJGSZsk3Vxi/dskPSLpIUn3SnphlvXk4cS4\ngudBMrOJL7NQkFQJ3AK8CnghcF2JD/0vRcRFEXEp8CHgr7OqJy/nzZlO8/QajyuY2aSQZU+hDdgU\nEZsjoge4HbimuEFEFM8BMZ3krm5TSvG4gpnZRJdlKCwAthc97kiXnUTSOyT9lKSncGOpDUm6XlK7\npPbOzs5Mis1SW2uBjn1H2bH/aN6lmJmNKMtQUIllz+kJRMQtEXEe8D+BPy21oYi4NSKWR8TyOXMm\n332PB8YV1m51b8HMJrYsQ6EDWFT0eCGwY4T2twO/kmE9ublgbgMza6s8rmBmE16WobAWWCqpVVIN\ncC2wuriBpKVFD18DPJlhPbmprBArWjyuYGYTX2ahEBF9wA3AXcDjwFcjYoOkD0i6Om12g6QNkh4C\n3g38Vlb15K2ttcCmZw+x+9CxvEsxMxtWVZYbj4g7gTuHLHtf0d83Zfn6E8nAuEL71r2sWjYv52rM\nzErzFc3jZNn8RuqqKz2uYGYTmkNhnNRUVfCiJU0eVzCzCc2hMI7aWpp5bOcBDnT35l2KmVlJDoVx\n1NZaIALWbd2XdylmZiU5FMbRZYubqK6UxxXMbMJyKIyj2upKLlnYxHcf2+VTU81sQnIojLO3XvU8\nOvYdZdVH7+HuJybfPE5mNrU5FMbZqmVz+bd3XMGs+mrefNsa/s+dj9PT1593WWZmgEMhFy+Y18Dq\nG67kDSsXc+vdm/m1v/0xW3YfzrssMzOHQl7qair54K9exGfedDnb9x3hNR+/h39u3+57OZtZrhwK\nOfvFC+fy7Zuu4qIFjfyPO9Zz4+0P+ToGM8uNQ2ECmNdYx5d+78X80S+cz52P7OTVH7uHddt8LYOZ\njT+HwgRRWSFu+LmlfPX3XwLA6z/zX3zi+09yvN+Hk8xs/DgUJpjLl8zizpuu4jUXzeOv/v0JfvOz\n97Gzy7fxNLPx4VCYgBpqq/nYtZfykV+/hEee7mLVR+/hO4/uyrssMysDDoUJShKvu3wh37rxKhYX\n6nnbP67jj7/+CEd7juddmplNYQ6FCa519nS+9vaX8vsvex5fuv8prv7kvTy+80DeZZnZFOVQmARq\nqip476tfwD/8bhv7j/ZyzS0/4gs/2uJrGszsrHMoTCJXLZ3Dt2+6iivOa+b933iMt36xnT2eWM/M\nziKHwiQze8Y0bnvLCt73Sy/knid386qP3cO9T+7OuywzmyIcCpOQJH7nyla+/o6XMrO2ijfddj//\n99ueWM/MzpxDYRK7cH4j3/iDK7l2xSI+85+bed2nf8xWT6xnZmfAoTDJ1ddU8X9fezGfesOL2Lr7\nMK/5+D38ywMdeZdlZpOUQ2GKePVF8/j2O1/GhfMbefdXH+adtz/IQU+sZ2anKNNQkLRK0kZJmyTd\nXGL9uyU9Jmm9pO9LWpJlPVPdgqY6vnz9i3nXK85n9cM7ePXH7+GBpzyxnpmNXWahIKkSuAV4FfBC\n4DpJLxzS7EFgeURcDNwBfCirespFZYW46RXJxHr9/fDrn/4vbvnBJk+sZ2ZjkmVPoQ3YFBGbI6IH\nuB24prhBRPwgIo6kD+8DFmZYT1lZ3lLgzpuuYtWyuXz4ro288XP3s6urO++yzGyCyzIUFgDbix53\npMuG87vAt0utkHS9pHZJ7Z2dvtn9WDXWVfPJ6y7jQ792MQ9t38+qj93Ndzd4Yj0zG16WoaASy0oe\nw5D0RmA58OFS6yPi1ohYHhHL58yZcxZLnPok8foVi/jmjVcyv7GO6/9hHf/rXx+lu9cT65nZc2UZ\nCh3AoqLHC4EdQxtJegXwJ8DVEeE5GzJy3pwZfP0dL+WtV7byD/dt4+pP3sv9m/fQ77EGMyuirCZV\nk1QFPAH8PPA0sBb4zYjYUNTmMpIB5lUR8eRYtrt8+fJob2/PoOLy8cONz/JH//wwuw/1MK+xllct\nm8drLp7HZYuaqKgo1cEzs8lO0rqIWD5quyxn2pT0auCjQCVwW0R8UNIHgPaIWC3pe8BFwM70KU9F\nxNUjbdOhcHYc7O7le48/w7fW7+LuJzrpOd7vgDCbwiZEKGTBoXD2Heju5fsOCLMpzaFgp8UBYTY1\nORTsjDkgzKYOh4KdVQ4Is8nNoWCZcUCYTT4OBRsXDgizycGhYOPOAWE2cTkULFcOCLOJxaFgE8bI\nATGXSxfNotIBYZYph4JNSKUCor6mkmXzG7loYSMXL2zkogWNtDRPd0/C7CxyKNiEd6C7lx/85Fke\nfGo/6zv2s2HHAY719QMwc1oVyxakIbGwkYsXNLGoUIfkoDA7HQ4Fm3T6jvfz5LOHeKSji/VP7+eR\nji4e33mQnuNJUDTWVQ/2JJKwaGJ+Y62DwmwMHAo2JfT09fPEMwdZ39HFI0/vZ31HFxt3HaQvnfK7\neXpN2pNIQuLihY2c21Cbc9VmE89YQ6FqPIoxO101VRUsW9DIsgWNwGIAunuP85NdB3mkY38aFl3c\n/UQnA7eGOGfmtLRH0TR4+Gn2jGn57YTZJOJQsEmntrqSSxc1cemipsFlR3uO89jOriQkOrpY/3QX\n3//Jswx0hOc31qYD2U1ctCA5BDVrek1Oe2A2cTkUbEqoq6nk8iUFLl9SGFx26FgfG55OehIDPYq7\nNjwzuH5RoY6LFzQNHn4675wZnDNzmscorKw5FGzKmjGtipXPa2bl85oHl3Ud7WXD00lPYmBA+1uP\n7BxcX1ddyZLmepY019PSPJ0lzdNpaa6nZfZ05jbU+jRZm/IcClZWGuuqeenzZ/PS588eXLbvcA8b\ndhxgy57DbNt9mK17jrC58zA/2NhJT3qKLCTjG0sK9YNBsWR2GhjN05nfVOcL8GxKcChY2Zs1vYYr\nl87myqWzT1re3x/sPNA9GBTb9hxm657DbNtzhHs3ddLdeyIwqivFosJA7+Lk3wtm1VFdWTHeu2V2\nWhwKZsOoqBALmupY0FTHS59/8rqI4JkDx9KQKAqN3Ue4f/MeDvccH2xbWSEWzqo7cSiqeTots5Me\nx6JZ9dRUOTBs4nAomJ0GScxtrGVuYy0vLhqzgCQwdh/qYeuew2zdnfQsBnoYD27bx8FjfYNtKwTz\nm+oGexZLmuuZ21jH3IZa5jbUck7DNGqrK8d796yMORTMzjJJzJk5jTkzp7GipXDSuohg35HewcA4\ncVjqCN96ZCf7j/Q+Z3uz6qs5tyEJoHmNtcnfDbWc21g7GB5N9dU+a8rOCoeC2TiSRGF6DYXpNbxo\n8aznrO862sszB7rZ1dXNrgPdPDPw+0Dy+9GnD7D70LHnPG9aVcVJYVEcHnMbp3FuQy3nzKz1oSob\nlUPBbAJprKumsa6a88+dOWybnr5+nj2YBkXXsROhkQbI+o79fHdD9+DkgsVmz6gpCoshPY40SBpq\nq9zrKGMOBbNJpqaqgoWz6lk4q37YNhFB19FedpbqcXR1s6Ormwe372fv4Z7nPLeuupJzG6ZRmF5D\n84xpNKc9m5P/rqF5etLGvY+pJdNQkLQK+BhQCXwuIv5yyPqXAR8FLgaujYg7sqzHrFxIoqm+hqb6\nGl4wr2HYdt29x+k8eOw54fHswWPsPXyM7XuP8ND2/ew73DM4CeFQM2urhgkOh8hklFkoSKoEbgFe\nCXQAayWtjojHipo9BbwF+KOs6jCz4dVWV7KoUM+iwvC9Dkh6HgeO9rH78DH2Hu5hz6Ee9hw+xt5D\nPew5nPxkESKzplczrcpnX42nLHsKbcCmiNgMIOl24BpgMBQiYmu67rkHP81swpBEY301jfXVnDdn\n9Pb9/cGB7t40LHrYc+hY8vdphEhtdQWNddU01dUkYy711YNjL01DHjfWVdNUn7RrqK2iyhcNnrIs\nQ2EBsL3ocQew8nQ2JOl64HqAxYsXn3llZpapiooTh6/OJET2H+ml62gv+4/0pL972b73CBuO9rL/\naC9Hii4SLGXmtCoa6qppKg6S+upk2UDIDFnfWF/NzGnlO9ieZSiUekdP644+EXErcCskN9k5k6LM\nbOI51RAZ0NPXT9fR3qKfJDy6jiShMfD3wPonnz00uGzgjn4l6xEn9T4a6qqZWVvFjGlVzKytTn9X\n0VBbzYzaqpPWNdRWMaO2irrqykkZLFmGQgewqOjxQmBHhq9nZmWmpqpi8ELBUxERdPcmgbL/aM9g\ncOw/2suBtEcyECT7j/ZysDs5k+tQdx8Hu3tPmsZkOJUVGgyPmbVJ72NmGhhJiFSnwZIum3YiYGam\n62bUVo37vFlZhsJaYKmkVuBp4FrgNzN8PTOzMZFEXU0ldTWVzG089du3Hu8PDh1LAiL53ceh7j4O\nFD0+2N2bhkgfB9O2uw50c6jzxPre46Mf+KitrhgMlXe+8nyuvmT+6ezymGUWChHRJ+kG4C6SU1Jv\ni4gNkj4AtEfEakkrgK8Ds4BflvTnEXFhVjWZmZ0NlRUaPLR0Jrp7jz8nRA50950InKJAOdjdx6z6\nM3u9sVDE5DpEv3z58mhvb8+7DDOzSUXSuohYPlo7n69lZmaDHApmZjbIoWBmZoMcCmZmNsihYGZm\ngxwKZmY2yKFgZmaDHApmZjYBly4SAAAHJklEQVRo0l28JqkT2HaaT58N7D6L5Ux2fj9O5vfjBL8X\nJ5sK78eSiBh1usFJFwpnQlL7WK7oKxd+P07m9+MEvxcnK6f3w4ePzMxskEPBzMwGlVso3Jp3AROM\n34+T+f04we/Fycrm/SirMQUzMxtZufUUzMxsBA4FMzMbVDahIGmVpI2SNkm6Oe968iJpkaQfSHpc\n0gZJN+Vd00QgqVLSg5K+mXcteZPUJOkOST9J/z95Sd415UXSu9J/J49K+rKkU7935yRTFqEgqRK4\nBXgV8ELgOkkvzLeq3PQBfxgRLwBeDLyjjN+LYjcBj+ddxATxMeA7EXEBcAll+r5IWgDcCCyPiGUk\ntxW+Nt+qslcWoQC0AZsiYnNE9AC3A9fkXFMuImJnRDyQ/n2Q5B/8gnyrypekhcBrgM/lXUveJDUA\nLwP+DiAieiJif75V5aoKqJNUBdQDO3KuJ3PlEgoLgO1Fjzso8w9CAEktwGXA/flWkruPAu8B+vMu\nZAJ4HtAJfD49nPY5SdPzLioPEfE08BHgKWAn0BUR3823quyVSyioxLKyPhdX0gzga8A7I+JA3vXk\nRdIvAc9GxLq8a5kgqoAXAX8bEZcBh4GyHIOTNIvkiEIrMB+YLumN+VaVvXIJhQ5gUdHjhZRBN3A4\nkqpJAuGfIuJf8q4nZ1cAV0vaSnJY8eck/WO+JeWqA+iIiIHe4x0kIVGOXgFsiYjOiOgF/gV4ac41\nZa5cQmEtsFRSq6QaksGi1TnXlAtJIjle/HhE/HXe9eQtIt4bEQsjooXk/4v/iIgp/21wOBGxC9gu\n6WfSRT8PPJZjSXl6CnixpPr0383PUwaD7lV5FzAeIqJP0g3AXSRnENwWERtyLisvVwBvAh6R9FC6\n7I8j4s4ca7KJ5Q+Af0q/QG0GfjvnenIREfdLugN4gOSsvQcpg+kuPM2FmZkNKpfDR2ZmNgYOBTMz\nG+RQMDOzQQ4FMzMb5FAwM7NBDgXLhKQfp79bJP3mWd72H5d6raxI+hVJ78to24cy2u7Lz3TGV0lf\nkPS6EdbfIKksT1edyhwKlomIGLjyswU4pVBIZ7UdyUmhUPRaWXkP8Kkz3cgY9itz6cRuZ8ttJLOI\n2hTiULBMFH0D/kvgKkkPpXPTV0r6sKS1ktZL+v20/cvT+zx8CXgkXfavktal89lfny77S5JZKx+S\n9E/Fr6XEh9O57x+R9BtF2/5h0T0C/im9QhVJfynpsbSWj5TYj/OBYxGxO338BUmflnSPpCfSuZMG\n7scwpv0q8RoflPSwpPsknVv0Oq8ranOoaHvD7cuqdNm9wGuLnvt+SbdK+i7w9yPUKkmfTN+PbwHn\nFG3jOe9TRBwBtkpqG8v/EzY5lMUVzZarm4E/ioiBD8/rSWabXCFpGvCj9MMKkinOl0XElvTx70TE\nXkl1wFpJX4uImyXdEBGXlnit1wKXktwDYHb6nLvTdZcBF5LMefUj4ApJjwG/ClwQESGpqcQ2ryC5\norVYC/DfgPOAH0h6PvDmU9ivYtOB+yLiTyR9CPg94H+XaFes1L60A58Ffg7YBHxlyHMuB66MiKMj\n/De4DPgZ4CLgXJLpLW6TVBjhfWoHrgLWjFKzTRLuKdh4+wXgzekUG/cDzcDSdN2aIR+cN0p6GLiP\nZELDpYzsSuDLEXE8Ip4B/hNYUbTtjojoBx4i+WA/AHQDn5P0WuBIiW3OI5lKuthXI6I/Ip4kmQbi\nglPcr2I9wMCx/3VpXaMptS8XkEze9mQk0xQMndRvdUQcTf8ertaXceL92wH8R9p+pPfpWZIZRG2K\ncE/BxpuAP4iIu05aKL2cZJrm4sevAF4SEUck/RAY7VaIpaZIH3Cs6O/jQFU6J1YbyURn1wI3kHzT\nLnYUaByybOjcMMEY96uE3jgx18xxTvyb7CP90pYeHqoZaV+GqatYcQ3D1frqUtsY5X2qJXmPbIpw\nT8GydhCYWfT4LuDtSqbvRtL5Kn0Tl0ZgXxoIF5DcOnRA78Dzh7gb+I30mPkckm++wx7WUHJPicZ0\nMsB3khx6Gupx4PlDlv26pApJ55HclGbjKezXWG0lOeQDyZz+pfa32E+A1rQmgOtGaDtcrXcD16bv\n3zzgZ9P1I71P5wOPjnmvbMJzT8Gyth7oSw8DfYHk/r8twAPpN+BO4FdKPO87wNskrSf50L2vaN2t\nwHpJD0TEG4qWfx14CfAwyTfe90TErjRUSpkJ/JuSm7ELeFeJNncDfyVJRd/oN5IcmjoXeFtEdEv6\n3Bj3a6w+m9a2Bvg+I/c2SGu4HviWpN3AvcCyYZoPV+vXSXoAjwBPpPsII79PVwB/fsp7ZxOWZ0k1\nG4WkjwHfiIjvSfoC8M2IuCPnsnIn6TLg3RHxprxrsbPHh4/MRvd/SG7abiebDfyvvIuws8s9BTMz\nG+SegpmZDXIomJnZIIeCmZkNciiYmdkgh4KZmQ36/75s1r2UEEyDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b4485c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
